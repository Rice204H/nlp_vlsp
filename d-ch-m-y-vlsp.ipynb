{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1769145",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:14.886256Z",
     "iopub.status.busy": "2025-12-11T16:37:14.885793Z",
     "iopub.status.idle": "2025-12-11T16:37:16.215413Z",
     "shell.execute_reply": "2025-12-11T16:37:16.214531Z"
    },
    "papermill": {
     "duration": 1.335601,
     "end_time": "2025-12-11T16:37:16.217019",
     "exception": false,
     "start_time": "2025-12-11T16:37:14.881418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/entovinlp/tst2012.vi\n",
      "/kaggle/input/entovinlp/train.vi\n",
      "/kaggle/input/entovinlp/tst2013.en\n",
      "/kaggle/input/entovinlp/tst2013.vi\n",
      "/kaggle/input/entovinlp/tst2012.en\n",
      "/kaggle/input/entovinlp/train.en\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1ded4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:16.224159Z",
     "iopub.status.busy": "2025-12-11T16:37:16.223884Z",
     "iopub.status.idle": "2025-12-11T16:37:19.591166Z",
     "shell.execute_reply": "2025-12-11T16:37:19.590344Z"
    },
    "papermill": {
     "duration": 3.372273,
     "end_time": "2025-12-11T16:37:19.592402",
     "exception": false,
     "start_time": "2025-12-11T16:37:16.220129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: SETUP ---\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import warnings\n",
    "\n",
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán tokenizers n·∫øu ch∆∞a c√≥\n",
    "try:\n",
    "    import tokenizers\n",
    "except ImportError:\n",
    "    os.system('pip install tokenizers')\n",
    "    import tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# C·∫•u h√¨nh thi·∫øt b·ªã v√† Random Seed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5ccfdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:19.599721Z",
     "iopub.status.busy": "2025-12-11T16:37:19.599117Z",
     "iopub.status.idle": "2025-12-11T16:37:19.605786Z",
     "shell.execute_reply": "2025-12-11T16:37:19.605227Z"
    },
    "papermill": {
     "duration": 0.01126,
     "end_time": "2025-12-11T16:37:19.606764",
     "exception": false,
     "start_time": "2025-12-11T16:37:19.595504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CELL 2: DATA READING FUNCTION ---\n",
    "def read_parallel_files(src_filename, tgt_filename):\n",
    "    \"\"\"ƒê·ªçc c·∫∑p file song ng·ªØ, tr·∫£ v·ªÅ list c√°c tuple (c√¢u_ngu·ªìn, c√¢u_ƒë√≠ch)\"\"\"\n",
    "    # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n (h·ªó tr·ª£ c·∫£ th∆∞ m·ª•c hi·ªán t·∫°i v√† th∆∞ m·ª•c input c·ªßa Kaggle)\n",
    "    possible_paths = [\"./\", \"/kaggle/input/\", \"/kaggle/working/\"]\n",
    "    \n",
    "    src_path, tgt_path = None, None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(os.path.join(p, src_filename)):\n",
    "            src_path = os.path.join(p, src_filename)\n",
    "        if os.path.exists(os.path.join(p, tgt_filename)):\n",
    "            tgt_path = os.path.join(p, tgt_filename)\n",
    "            \n",
    "    if not src_path or not tgt_path:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {src_filename} ho·∫∑c {tgt_filename}. B·ªè qua.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc: {src_path} v√† {tgt_path}\")\n",
    "    with open(src_path, 'r', encoding='utf-8') as f_src, \\\n",
    "         open(tgt_path, 'r', encoding='utf-8') as f_tgt:\n",
    "        src_lines = [line.strip() for line in f_src.read().splitlines()]\n",
    "        tgt_lines = [line.strip() for line in f_tgt.read().splitlines()]\n",
    "    \n",
    "    # L·ªçc b·ªè c√°c c·∫∑p c√¢u r·ªóng ho·∫∑c l·ªách d√≤ng\n",
    "    pairs = []\n",
    "    min_len = min(len(src_lines), len(tgt_lines))\n",
    "    for i in range(min_len):\n",
    "        if src_lines[i] and tgt_lines[i]:\n",
    "            pairs.append((src_lines[i], tgt_lines[i]))\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd817be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:19.613247Z",
     "iopub.status.busy": "2025-12-11T16:37:19.612736Z",
     "iopub.status.idle": "2025-12-11T16:37:20.232035Z",
     "shell.execute_reply": "2025-12-11T16:37:20.231076Z"
    },
    "papermill": {
     "duration": 0.623847,
     "end_time": "2025-12-11T16:37:20.233294",
     "exception": false,
     "start_time": "2025-12-11T16:37:19.609447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ƒêANG T·∫¢I D·ªÆ LI·ªÜU ---\n",
      "üìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/train.en v√† /kaggle/input/entovinlp/train.vi\n",
      "üìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/tst2012.en v√† /kaggle/input/entovinlp/tst2012.vi\n",
      "üìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/tst2013.en v√† /kaggle/input/entovinlp/tst2013.vi\n",
      "‚úÖ Train size: 133166\n",
      "‚úÖ Val size: 1553\n",
      "‚úÖ Test size: 1268\n",
      "üîé V√≠ d·ª• m·∫´u: ('Rachel Pike : The science behind a climate headline', 'Khoa h·ªçc ƒë·∫±ng sau m·ªôt ti√™u ƒë·ªÅ v·ªÅ kh√≠ h·∫≠u')\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 3: LOAD DATA ---\n",
    "print(\"\\n--- ƒêANG T·∫¢I D·ªÆ LI·ªÜU ---\")\n",
    "# ƒê·∫£m b·∫£o t√™n file kh·ªõp v·ªõi file b·∫°n upload\n",
    "train_pairs = read_parallel_files(\"/kaggle/input/entovinlp/train.en\", \"/kaggle/input/entovinlp/train.vi\")\n",
    "val_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2012.en\", \"/kaggle/input/entovinlp/tst2012.vi\")\n",
    "test_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2013.en\", \"/kaggle/input/entovinlp/tst2013.vi\")\n",
    "\n",
    "print(f\"‚úÖ Train size: {len(train_pairs)}\")\n",
    "print(f\"‚úÖ Val size: {len(val_pairs)}\")\n",
    "print(f\"‚úÖ Test size: {len(test_pairs)}\")\n",
    "\n",
    "if len(train_pairs) > 0:\n",
    "    print(f\"üîé V√≠ d·ª• m·∫´u: {train_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10f2c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:20.240770Z",
     "iopub.status.busy": "2025-12-11T16:37:20.240141Z",
     "iopub.status.idle": "2025-12-11T16:37:26.591718Z",
     "shell.execute_reply": "2025-12-11T16:37:26.590854Z"
    },
    "papermill": {
     "duration": 6.357358,
     "end_time": "2025-12-11T16:37:26.593841",
     "exception": false,
     "start_time": "2025-12-11T16:37:20.236483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HU·∫§N LUY·ªÜN TOKENIZER ---\n",
      "‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: TRAIN TOKENIZERS ---\n",
    "print(\"\\n--- HU·∫§N LUY·ªÜN TOKENIZER ---\")\n",
    "\n",
    "def train_bpe_tokenizer(texts, vocab_size=8000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[UNK]\"],\n",
    "        show_progress=False\n",
    "    )\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# G·ªôp text ƒë·ªÉ train tokenizer\n",
    "all_src_text = [p[0] for p in train_pairs + val_pairs]\n",
    "all_tgt_text = [p[1] for p in train_pairs + val_pairs]\n",
    "\n",
    "if not all_src_text: # Dummy data n·∫øu ch∆∞a load ƒë∆∞·ª£c file\n",
    "    all_src_text = [\"Hello world\"]\n",
    "    all_tgt_text = [\"Xin ch√†o\"]\n",
    "\n",
    "en_tokenizer = train_bpe_tokenizer(all_src_text, vocab_size=10000)\n",
    "vi_tokenizer = train_bpe_tokenizer(all_tgt_text, vocab_size=10000)\n",
    "\n",
    "# L·∫•y ID c√°c token ƒë·∫∑c bi·ªát\n",
    "PAD_ID = en_tokenizer.token_to_id(\"[PAD]\")\n",
    "START_ID = vi_tokenizer.token_to_id(\"[START]\")\n",
    "END_ID = vi_tokenizer.token_to_id(\"[END]\")\n",
    "\n",
    "print(\"‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7480756e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:26.605413Z",
     "iopub.status.busy": "2025-12-11T16:37:26.605071Z",
     "iopub.status.idle": "2025-12-11T16:37:26.613376Z",
     "shell.execute_reply": "2025-12-11T16:37:26.612697Z"
    },
    "papermill": {
     "duration": 0.01561,
     "end_time": "2025-12-11T16:37:26.614724",
     "exception": false,
     "start_time": "2025-12-11T16:37:26.599114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CELL 5: DATASET CLASS ---\n",
    "class EnViDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    en_batch, vi_batch = zip(*batch)\n",
    "    \n",
    "    # Encode ti·∫øng Anh (Source)\n",
    "    en_enc = en_tokenizer.encode_batch(list(en_batch))\n",
    "    en_ids = [e.ids for e in en_enc]\n",
    "    \n",
    "    # Encode ti·∫øng Vi·ªát (Target) - Th√™m START v√† END th·ªß c√¥ng\n",
    "    vi_ids = []\n",
    "    for text in vi_batch:\n",
    "        ids = vi_tokenizer.encode(text).ids\n",
    "        vi_ids.append([START_ID] + ids + [END_ID])\n",
    "    \n",
    "    # Padding\n",
    "    max_len_en = max([len(x) for x in en_ids])\n",
    "    max_len_vi = max([len(x) for x in vi_ids])\n",
    "    \n",
    "    padded_en = [x + [PAD_ID] * (max_len_en - len(x)) for x in en_ids]\n",
    "    padded_vi = [x + [PAD_ID] * (max_len_vi - len(x)) for x in vi_ids]\n",
    "    \n",
    "    return torch.tensor(padded_en), torch.tensor(padded_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f07f962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:26.625214Z",
     "iopub.status.busy": "2025-12-11T16:37:26.624907Z",
     "iopub.status.idle": "2025-12-11T16:37:26.630505Z",
     "shell.execute_reply": "2025-12-11T16:37:26.629854Z"
    },
    "papermill": {
     "duration": 0.012073,
     "end_time": "2025-12-11T16:37:26.631496",
     "exception": false,
     "start_time": "2025-12-11T16:37:26.619423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created. Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 6: DATALOADERS ---\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(EnViDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(EnViDataset(val_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"DataLoaders created. Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d37c288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:26.638511Z",
     "iopub.status.busy": "2025-12-11T16:37:26.638334Z",
     "iopub.status.idle": "2025-12-11T16:37:26.656530Z",
     "shell.execute_reply": "2025-12-11T16:37:26.656007Z"
    },
    "papermill": {
     "duration": 0.023016,
     "end_time": "2025-12-11T16:37:26.657475",
     "exception": false,
     "start_time": "2025-12-11T16:37:26.634459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CELL 7: MODEL COMPONENTS ---\n",
    "# --- Rotary Positional Embeddings ---\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos\", emb.cos()[None, :, None, :])\n",
    "        self.register_buffer(\"sin\", emb.sin()[None, :, None, :])\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        return self.cos[:, :seq_len, :, :], self.sin[:, :seq_len, :, :]\n",
    "\n",
    "# --- Feed Forward (SwiGLU) ---\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.w3 = nn.Linear(intermediate_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "# --- GQA Attention ---\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_groups = num_heads // num_kv_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, enc_out=None, mask=None, rope_cos=None, rope_sin=None):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        kv_input = enc_out if enc_out is not None else x\n",
    "        kv_seq_len = kv_input.shape[1]\n",
    "\n",
    "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        if rope_cos is not None and enc_out is None:\n",
    "            q = apply_rotary_pos_emb(q, rope_cos, rope_sin)\n",
    "            k = apply_rotary_pos_emb(k, rope_cos, rope_sin)\n",
    "\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if self.num_groups > 1:\n",
    "            k = k[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n",
    "            v = v[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0)\n",
    "        return self.o_proj(out.transpose(1, 2).reshape(batch, seq_len, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6cf260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:26.664220Z",
     "iopub.status.busy": "2025-12-11T16:37:26.664026Z",
     "iopub.status.idle": "2025-12-11T16:37:26.672257Z",
     "shell.execute_reply": "2025-12-11T16:37:26.671757Z"
    },
    "papermill": {
     "duration": 0.012704,
     "end_time": "2025-12-11T16:37:26.673183",
     "exception": false,
     "start_time": "2025-12-11T16:37:26.660479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CELL 8: TRANSFORMER MODEL ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(hidden_dim)\n",
    "        self.attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.is_decoder = is_decoder\n",
    "        if is_decoder:\n",
    "            self.norm2 = nn.RMSNorm(hidden_dim)\n",
    "            self.cross_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.norm_ffn = nn.RMSNorm(hidden_dim)\n",
    "        self.ffn = SwiGLU(hidden_dim, hidden_dim * 4)\n",
    "\n",
    "    def forward(self, x, enc_out=None, mask=None, cross_mask=None, rope_cos=None, rope_sin=None):\n",
    "        x = x + self.attn(self.norm1(x), mask=mask, rope_cos=rope_cos, rope_sin=rope_sin)\n",
    "        if self.is_decoder:\n",
    "            x = x + self.cross_attn(self.norm2(x), enc_out=enc_out, mask=cross_mask)\n",
    "        x = x + self.ffn(self.norm_ffn(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab, hidden_dim)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, hidden_dim)\n",
    "        self.rope = RotaryPositionalEncoding(hidden_dim // num_heads)\n",
    "        self.encoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads) for _ in range(num_layers)])\n",
    "        self.decoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads, is_decoder=True) for _ in range(num_layers)])\n",
    "        self.final_norm = nn.RMSNorm(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, tgt_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        x = self.src_emb(src)\n",
    "        rope_cos, rope_sin = self.rope(x, x.shape[1])\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x, mask=src_mask, rope_cos=rope_cos, rope_sin=rope_sin)\n",
    "        enc_out = x\n",
    "        \n",
    "        x = self.tgt_emb(tgt)\n",
    "        rope_cos_tgt, rope_sin_tgt = self.rope(x, x.shape[1])\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x, enc_out=enc_out, mask=tgt_mask, cross_mask=src_mask, rope_cos=rope_cos_tgt, rope_sin=rope_sin_tgt)\n",
    "        return self.fc_out(self.final_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6a9b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:26.679622Z",
     "iopub.status.busy": "2025-12-11T16:37:26.679429Z",
     "iopub.status.idle": "2025-12-11T16:37:31.184750Z",
     "shell.execute_reply": "2025-12-11T16:37:31.184076Z"
    },
    "papermill": {
     "duration": 4.509875,
     "end_time": "2025-12-11T16:37:31.185901",
     "exception": false,
     "start_time": "2025-12-11T16:37:26.676026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 9: INIT TRAINING ---\n",
    "def create_masks(src, tgt):\n",
    "    src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "    batch, seq_len = tgt.shape\n",
    "    causal = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n",
    "    tgt_pad = (tgt == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "    return src_mask, causal + tgt_pad\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab=en_tokenizer.get_vocab_size(),\n",
    "    tgt_vocab=vi_tokenizer.get_vocab_size(),\n",
    "    hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "print(\"Model Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7a0d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:37:31.193183Z",
     "iopub.status.busy": "2025-12-11T16:37:31.192899Z",
     "iopub.status.idle": "2025-12-11T17:50:48.329131Z",
     "shell.execute_reply": "2025-12-11T17:50:48.328271Z"
    },
    "papermill": {
     "duration": 4397.141408,
     "end_time": "2025-12-11T17:50:48.330580",
     "exception": false,
     "start_time": "2025-12-11T16:37:31.189172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.50it/s, loss=3.7220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.3791 | Val Loss: 3.8044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.50it/s, loss=3.3583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 3.4783 | Val Loss: 3.5313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:20<00:00,  9.45it/s, loss=3.1177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.2081 | Val Loss: 3.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:16<00:00,  9.53it/s, loss=2.9075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.0505 | Val Loss: 3.3811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.50it/s, loss=2.9385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 2.9347 | Val Loss: 3.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.49it/s, loss=2.9381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 2.8409 | Val Loss: 3.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.49it/s, loss=2.7164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 2.7602 | Val Loss: 3.3313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.49it/s, loss=2.4812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 2.6890 | Val Loss: 3.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:18<00:00,  9.49it/s, loss=2.5523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 2.6261 | Val Loss: 3.3468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:19<00:00,  9.47it/s, loss=2.7272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 2.5690 | Val Loss: 3.3699\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 10: TRAINING LOOP ---\n",
    "EPOCHS = 10\n",
    "print(\"\\n--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for src, tgt in pbar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n",
    "        src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            val_loss += criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1)).item()\n",
    "            \n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"transformer_en_vi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a1e3b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T17:50:53.762818Z",
     "iopub.status.busy": "2025-12-11T17:50:53.762431Z",
     "iopub.status.idle": "2025-12-11T17:50:54.763251Z",
     "shell.execute_reply": "2025-12-11T17:50:54.762595Z"
    },
    "papermill": {
     "duration": 3.68989,
     "end_time": "2025-12-11T17:50:54.764318",
     "exception": false,
     "start_time": "2025-12-11T17:50:51.074428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST K·∫æT QU·∫¢ ---\n",
      "üîπ Input:  And he said that he needed those guns because of the trauma he &apos;d experienced as a young boy .\n",
      "üî∏ Target: V√† anh ta n√≥i r·∫±ng anh ta c·∫ßn nh·ªØng c√¢y s√∫ng n√†y b·ªüi v√¨ nh·ªØng t·ªïn th∆∞∆°ng m√† anh ƒë√£ tr·∫£i qua trong qu√° kh·ª© khi l√† m·ªôt ƒë·ª©a tr·∫ª .\n",
      "üöÄ Model:   V√† √¥ng n√≥i r·∫±ng √¥ng c·∫ßn s√∫ng ƒë√≥ b·ªüi v√¨ nh·ªØng ch·∫•n th∆∞∆°ng tinh m√† √¥ng ƒë√£ tr·∫£i qua khi c√≤n l√† m·ªôt c·∫≠u b√© .\n",
      "--------------------------------------------------\n",
      "üîπ Input:  Am I South Korean or North Korean ?\n",
      "üî∏ Target: T√¥i l√† ng∆∞·ªùi Nam Tri·ªÅu Ti√™n hay B·∫Øc Tri·ªÅu Ti√™n ?\n",
      "üöÄ Model:   T√¥i l√† ng∆∞·ªùi H√†n Qu·ªëc hay B·∫Øc Tri·ªÅu Ti√™n hay B·∫Øc Tri·ªÅu Ti√™n ?\n",
      "--------------------------------------------------\n",
      "üîπ Input:  And so just as the womb entirely envelopes the embryo , which grows within it , the divine matrix of compassion nourishes the entire existence .\n",
      "üî∏ Target: V√† b·ªüi v√¨ t·ª≠ cung bao b·ªçc ho√†n to√†n ph√¥i thai ƒëang ph√°t tri·ªÉn trong l√≤ng n√≥ , ma tr·∫≠n thi√™ng li√™ng c·ªßa t√¨nh th∆∞∆°ng nu√¥i d∆∞·ª°ng to√†n b·ªô s·ª± s·ªëng ƒë√≥ .\n",
      "üöÄ Model:   V√† v√¨ th·∫ø , khi lo√†i ki·∫øn th·ª£ thi·∫øu ni√™n ƒë·∫°i m·ªõi ph√°t tri·ªÉn trong n√≥ , ma tr·∫≠n ma thu·∫≠t c·ªßa l√≤ng tr·∫Øc ·∫©n v√† to√†n b·ªô .\n",
      "--------------------------------------------------\n",
      "üîπ Input:  It &apos;s really become sacred to us .\n",
      "üî∏ Target: N√≥ tr·ªü n√™n th·∫≠t thi√™ng li√™ng v·ªõi ch√∫ng t√¥i .\n",
      "üöÄ Model:   N√≥ th·ª±c s·ª± tr·ªü n√™n thi√™ng li√™ng ƒë·ªëi v·ªõi ch√∫ng ta .\n",
      "--------------------------------------------------\n",
      "üîπ Input:  This is a visualization of six months of my life .\n",
      "üî∏ Target: ƒê√¢y l√† nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ cu·ªôc s·ªëng trong s√°u th√°ng ƒë√£ ƒë∆∞·ª£c ghi l·∫°i c·ªßa t√¥i .\n",
      "üöÄ Model:   ƒê√¢y l√† h√¨nh ·∫£nh ho√° 6 th√°ng trong cu·ªôc ƒë·ªùi t√¥i .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 11: INFERENCE ---\n",
    "print(\"\\n--- TEST K·∫æT QU·∫¢ ---\")\n",
    "def translate(sentence, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = torch.tensor([en_tokenizer.encode(sentence).ids]).to(device)\n",
    "        src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "        tgt_ids = [START_ID]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt = torch.tensor([tgt_ids]).to(device)\n",
    "            causal = torch.triu(torch.full((tgt.shape[1], tgt.shape[1]), float('-inf'), device=device), diagonal=1)\n",
    "            out = model(src, tgt, src_mask, causal)\n",
    "            next_token = out[0, -1, :].argmax().item()\n",
    "            if next_token == END_ID: break\n",
    "            tgt_ids.append(next_token)\n",
    "        return vi_tokenizer.decode(tgt_ids[1:])\n",
    "\n",
    "for i in range(5):\n",
    "    if len(test_pairs) > 0:\n",
    "        idx = random.randint(0, len(test_pairs)-1)\n",
    "        en_txt, vi_txt = test_pairs[idx]\n",
    "        print(f\"üîπ Input:  {en_txt}\\nüî∏ Target: {vi_txt}\\nüöÄ Model:  {translate(en_txt)}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db64da93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T17:51:00.300199Z",
     "iopub.status.busy": "2025-12-11T17:51:00.299932Z",
     "iopub.status.idle": "2025-12-11T17:51:00.379694Z",
     "shell.execute_reply": "2025-12-11T17:51:00.378723Z"
    },
    "papermill": {
     "duration": 2.883356,
     "end_time": "2025-12-11T17:51:00.380928",
     "exception": false,
     "start_time": "2025-12-11T17:50:57.497572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Custom Test Input: I really like this model.\n",
      "‚úÖ Model Translation:  T√¥i r·∫•t th√≠ch m√¥ h√¨nh n√†y .\n"
     ]
    }
   ],
   "source": [
    "# Test th·ªß c√¥ng 1 c√¢u ri√™ng\n",
    "custom_sentence = \"I really like this model.\"\n",
    "print(f\"\\nüöÄ Custom Test Input: {custom_sentence}\")\n",
    "print(f\"‚úÖ Model Translation: {translate(custom_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ddd164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T17:51:05.777501Z",
     "iopub.status.busy": "2025-12-11T17:51:05.776955Z",
     "iopub.status.idle": "2025-12-11T17:51:10.107669Z",
     "shell.execute_reply": "2025-12-11T17:51:10.106941Z"
    },
    "papermill": {
     "duration": 7.022596,
     "end_time": "2025-12-11T17:51:10.108925",
     "exception": false,
     "start_time": "2025-12-11T17:51:03.086329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.11.3)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: portalocker, sacrebleu\r\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae50a70f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T17:51:15.508513Z",
     "iopub.status.busy": "2025-12-11T17:51:15.507963Z",
     "iopub.status.idle": "2025-12-11T17:51:43.009283Z",
     "shell.execute_reply": "2025-12-11T17:51:43.008448Z"
    },
    "papermill": {
     "duration": 30.197169,
     "end_time": "2025-12-11T17:51:43.010399",
     "exception": false,
     "start_time": "2025-12-11T17:51:12.813230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìä ƒêANG T√çNH ƒêI·ªÇM BLEU TR√äN 100 M·∫™U ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:27<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü ƒêI·ªÇM BLEU C·ª¶A MODEL: 27.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "import random\n",
    "from tqdm import tqdm # Thanh hi·ªÉn th·ªã ti·∫øn ƒë·ªô\n",
    "\n",
    "def calculate_bleu(data_pairs, num_samples=100):\n",
    "    print(f\"--- üìä ƒêANG T√çNH ƒêI·ªÇM BLEU TR√äN {num_samples} M·∫™U ---\")\n",
    "    \n",
    "    # Ch·ªçn ng·∫´u nhi√™n m·∫´u ƒë·ªÉ test (ho·∫∑c l·∫•y h·∫øt n·∫øu num_samples=None)\n",
    "    if num_samples is not None and num_samples < len(data_pairs):\n",
    "        samples = random.sample(data_pairs, num_samples)\n",
    "    else:\n",
    "        samples = data_pairs\n",
    "\n",
    "    preds = [] # C√°c c√¢u m√°y d·ªãch\n",
    "    refs = []  # C√°c c√¢u ƒë√°p √°n chu·∫©n\n",
    "\n",
    "    # B·∫Øt ƒë·∫ßu d·ªãch\n",
    "    for en_txt, vi_txt in tqdm(samples):\n",
    "        # D·ªãch c√¢u ti·∫øng Anh\n",
    "        pred_sent = translate(en_txt)\n",
    "        \n",
    "        preds.append(pred_sent)\n",
    "        refs.append(vi_txt) # Sacrebleu nh·∫≠n list c√°c string cho refs\n",
    "\n",
    "    # T√≠nh ƒëi·ªÉm BLEU\n",
    "    # refs c·∫ßn ƒë∆∞·ª£c b·ªçc trong list v√¨ 1 c√¢u input c√≥ th·ªÉ c√≥ nhi·ªÅu c√¢u target (·ªü ƒë√¢y ta c√≥ 1)\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [refs])\n",
    "    \n",
    "    return bleu.score\n",
    "\n",
    "# --- CH·∫†Y T√çNH ƒêI·ªÇM ---\n",
    "# B·∫°n c√≥ th·ªÉ tƒÉng s·ªë l∆∞·ª£ng m·∫´u l√™n len(test_pairs) ƒë·ªÉ ch√≠nh x√°c h∆°n (s·∫Ω ch·∫°y l√¢u h∆°n)\n",
    "score = calculate_bleu(test_pairs, num_samples=100)\n",
    "\n",
    "print(f\"\\nüåü ƒêI·ªÇM BLEU C·ª¶A MODEL: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8974101,
     "sourceId": 14093038,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4475.959693,
   "end_time": "2025-12-11T17:51:47.357171",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-11T16:37:11.397478",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
