{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14093038,"sourceType":"datasetVersion","datasetId":8974101}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/luuvnhng/d-ch-m-y-transformer?scriptVersionId=285964383\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:48.81309Z","iopub.execute_input":"2025-12-13T14:13:48.813373Z","iopub.status.idle":"2025-12-13T14:13:49.085505Z","shell.execute_reply.started":"2025-12-13T14:13:48.813351Z","shell.execute_reply":"2025-12-13T14:13:49.084919Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/entovinlp/tst2012.vi\n/kaggle/input/entovinlp/train.vi\n/kaggle/input/entovinlp/tst2013.en\n/kaggle/input/entovinlp/tst2013.vi\n/kaggle/input/entovinlp/tst2012.en\n/kaggle/input/entovinlp/train.en\n/kaggle/input/medicaldata/public_test.en.txt\n/kaggle/input/medicaldata/train.en.txt\n/kaggle/input/medicaldata/train.vi.txt\n/kaggle/input/medicaldata/public_test.vi.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# X√¢y d·ª±ng M√¥ h√¨nh D·ªãch m√°y b·∫±ng Transformer","metadata":{}},{"cell_type":"markdown","source":"# Thi·∫øt l·∫≠p M√¥i tr∆∞·ªùng & Th∆∞ vi·ªán\nKh·ªüi t·∫°o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (Torch, Tokenizers) v√† thi·∫øt l·∫≠p `SEED` c·ªë ƒë·ªãnh ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ c√≥ th·ªÉ t√°i l·∫≠p (reproducible) trong c√°c l·∫ßn ch·∫°y kh√°c nhau.\nKi·ªÉm tra GPU: S·ª≠ d·ª•ng CUDA n·∫øu c√≥ ƒë·ªÉ tƒÉng t·ªëc hu·∫•n luy·ªán.","metadata":{}},{"cell_type":"code","source":"# --- CELL 1: SETUP ---\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport tqdm\nimport warnings\n\n# C√†i ƒë·∫∑t th∆∞ vi·ªán tokenizers n·∫øu ch∆∞a c√≥\ntry:\n    import tokenizers\nexcept ImportError:\n    os.system('pip install tokenizers')\n    import tokenizers\n\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n\n# C·∫•u h√¨nh thi·∫øt b·ªã v√† Random Seed\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:49.086796Z","iopub.execute_input":"2025-12-13T14:13:49.087074Z","iopub.status.idle":"2025-12-13T14:13:52.373875Z","shell.execute_reply.started":"2025-12-13T14:13:49.087053Z","shell.execute_reply":"2025-12-13T14:13:52.373202Z"}},"outputs":[{"name":"stdout","text":"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu (Data Preprocessing)\nH√†m `clean_text` th·ª±c hi·ªán chu·∫©n h√≥a d·ªØ li·ªáu vƒÉn b·∫£n th√¥:\n- **Unicode Normalization:** Chuy·ªÉn v·ªÅ chu·∫©n NFC (quan tr·ªçng cho ti·∫øng Vi·ªát).\n- **Lowercase:** Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn.\n- **Contraction Expansion:** T√°ch c√°c t·ª´ vi·∫øt t·∫Øt ti·∫øng Anh (v√≠ d·ª•: \"I'm\" -> \"I am\", \"don't\" -> \"do not\") ƒë·ªÉ m√¥ h√¨nh h·ªçc ng·ªØ ph√°p t·ªët h∆°n.\n- **HTML Decoding:** X·ª≠ l√Ω c√°c k√Ω t·ª± m√£ h√≥a HTML.","metadata":{}},{"cell_type":"code","source":"import re\nimport html\nimport unicodedata\n\"\"\"\n    H√†m l√†m s·∫°ch c∆° b·∫£n, d√πng CHUNG cho c·∫£ train.vi, train.en v√† c√¢u input khi d·ªãch.\n    M·ª•c ti√™u:\n      - B·ªè kho·∫£ng tr·∫Øng th·ª´a, k√Ω t·ª± v√¥ h√¨nh\n      - Gi·∫£i m√£ HTML (&quot; -> \")\n      - Chu·∫©n h√≥a Unicode NFC (d·∫•u ti·∫øng Vi·ªát)\n      - ƒê∆∞a v·ªÅ ch·ªØ th∆∞·ªùng (r·∫•t quan tr·ªçng v√¨ Field(lower=True))\n      - Chu·∫©n h√≥a d·∫•u c√¢u: t√°ch , . ! ? ; : () \" ' ra kh·ªèi t·ª´\n    \"\"\"\ndef clean_text(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n\n    text = text.strip()\n    text = unicodedata.normalize(\"NFC\", text)\n    text = text.lower()\n    text = re.sub(r\"<[^>]+>\", \" \", text)\n    if not text:\n        return \"\"\n\n    # 1. Gi·∫£i m√£ html\n    text = html.unescape(text)\n\n    # 2. Expand contraction (nh∆∞ anh ƒë√£ vi·∫øt)\n    split_patterns = [\n        # BE (am/is/are)\n        (r\"\\bi\\s*m\\b\", \"i am\"),\n        (r\"\\byou\\s*re\\b\", \"you are\"),\n        (r\"\\bwe\\s*re\\b\", \"we are\"),\n        (r\"\\bthey\\s*re\\b\", \"they are\"),\n        (r\"\\bhe\\s*s\\b\", \"he is\"),\n        (r\"\\bshe\\s*s\\b\", \"she is\"),\n        (r\"\\bit\\s*s\\b\", \"it is\"),\n        (r\"\\bthat\\s*s\\b\", \"that is\"),\n        (r\"\\bthere\\s*s\\b\", \"there is\"),\n        (r\"\\bhere\\s*s\\b\", \"here is\"),\n    \n        # HAVE\n        (r\"\\bi\\s*ve\\b\", \"i have\"),\n        (r\"\\byou\\s*ve\\b\", \"you have\"),\n        (r\"\\bwe\\s*ve\\b\", \"we have\"),\n        (r\"\\bthey\\s*ve\\b\", \"they have\"),\n    \n        # WILL\n        (r\"\\bi\\s*ll\\b\", \"i will\"),\n        (r\"\\byou\\s*ll\\b\", \"you will\"),\n        (r\"\\bwe\\s*ll\\b\", \"we will\"),\n        (r\"\\bthey\\s*ll\\b\", \"they will\"),\n        (r\"\\bhe\\s*ll\\b\", \"he will\"),\n        (r\"\\bshe\\s*ll\\b\", \"she will\"),\n        (r\"\\bit\\s*ll\\b\", \"it will\"),\n    \n        # WOULD\n        (r\"\\bi\\s*d\\b\", \"i would\"),\n        (r\"\\byou\\s*d\\b\", \"you would\"),\n        (r\"\\bwe\\s*d\\b\", \"we would\"),\n        (r\"\\bthey\\s*d\\b\", \"they would\"),\n        (r\"\\bhe\\s*d\\b\", \"he would\"),\n        (r\"\\bshe\\s*d\\b\", \"she would\"),\n        (r\"\\bit\\s*d\\b\", \"it would\"),\n        (r\"\\bthat\\s*d\\b\", \"that would\"),\n    \n        # NOT (full)\n        (r\"\\bdon\\s*t\\b\", \"do not\"),\n        (r\"\\bdoesn\\s*t\\b\", \"does not\"),\n        (r\"\\bdidn\\s*t\\b\", \"did not\"),\n        (r\"\\bcan\\s*t\\b\", \"can not\"),\n        (r\"\\bcouldn\\s*t\\b\", \"could not\"),\n        (r\"\\bshouldn\\s*t\\b\", \"should not\"),\n        (r\"\\bwouldn\\s*t\\b\", \"would not\"),\n        (r\"\\bisn\\s*t\\b\", \"is not\"),\n        (r\"\\baren\\s*t\\b\", \"are not\"),\n        (r\"\\bwasn\\s*t\\b\", \"was not\"),\n        (r\"\\bweren\\s*t\\b\", \"were not\"),\n        (r\"\\bwon\\s*t\\b\", \"will not\"),\n        (r\"\\bmustn\\s*t\\b\", \"must not\"),\n        (r\"\\bneedn\\s*t\\b\", \"need not\"),\n        (r\"\\bmightn\\s*t\\b\", \"might not\"),\n    \n        # OTHER\n        (r\"\\blet\\s*s\\b\", \"let us\"),\n    ]\n\n    for pat, repl in split_patterns:\n        text = re.sub(pat, repl, text)\n\n    full_patterns = [\n        # BE\n        (r\"\\bi m\\b\", \"i am\"),\n        (r\"\\byou re\\b\", \"you are\"),\n        (r\"\\bwe re\\b\", \"we are\"),\n        (r\"\\bthey re\\b\", \"they are\"),\n        (r\"\\bhe s\\b\", \"he is\"),\n        (r\"\\bshe s\\b\", \"she is\"),\n        (r\"\\bit s\\b\", \"it is\"),\n        (r\"\\bthat s\\b\", \"that is\"),\n        (r\"\\bhere s\\b\", \"here is\"),\n        (r\"\\bthere s\\b\", \"there is\"),\n    \n        # HAVE\n        (r\"\\bi ve\\b\", \"i have\"),\n        (r\"\\byou ve\\b\", \"you have\"),\n        (r\"\\bwe ve\\b\", \"we have\"),\n        (r\"\\bthey ve\\b\", \"they have\"),\n\n        # WILL\n        (r\"\\bi ll\\b\", \"I will\"),\n        (r\"\\byou ll\\b\", \"You will\"),\n        (r\"\\bwe ll\\b\", \"We will\"),\n        (r\"\\bthey ll\\b\", \"They will\"),\n        (r\"\\bhe ll\\b\", \"He will\"),\n        (r\"\\bshe ll\\b\", \"She will\"),\n        (r\"\\bit ll\\b\", \"It will\"),\n    \n        # WOULD\n        (r\"\\bi d\\b\", \"I would\"),\n        (r\"\\byou d\\b\", \"You would\"),\n        (r\"\\bwe d\\b\", \"We would\"),\n        (r\"\\bthey d\\b\", \"They would\"),\n        (r\"\\bhe d\\b\", \"He would\"),\n        (r\"\\bshe d\\b\", \"She would\"),\n        (r\"\\bit d\\b\", \"It would\"),\n        (r\"\\bthat d\\b\", \"That would\"),\n    \n        # NOT\n        (r\"\\bdon t\\b\", \"do not\"),\n        (r\"\\bdoesn t\\b\", \"does not\"),\n        (r\"\\bdidn t\\b\", \"did not\"),\n        (r\"\\bcan t\\b\", \"can not\"),\n        (r\"\\bcouldn t\\b\", \"could not\"),\n        (r\"\\bshouldn t\\b\", \"should not\"),\n        (r\"\\bwouldn t\\b\", \"would not\"),\n        (r\"\\bisn t\\b\", \"is not\"),\n        (r\"\\baren t\\b\", \"are not\"),\n        (r\"\\bwasn t\\b\", \"was not\"),\n        (r\"\\bweren t\\b\", \"were not\"),\n        (r\"\\bwon t\\b\", \"will not\"),\n        (r\"\\bmustn t\\b\", \"must not\"),\n        (r\"\\bneedn t\\b\", \"need not\"),\n        (r\"\\bmightn t\\b\", \"might not\"),\n    \n        # OTHER\n        (r\"\\blet s\\b\", \"let us\"),\n    ]\n\n    for pat, repl in full_patterns:\n        text = re.sub(pat, repl, text)\n\n    # 3. B·ªè k√Ω t·ª± v√¥ h√¨nh + b·ªè --, ---, ...\n    text = text.replace(\"\\u200b\", \"\")\n    text = re.sub(r\"-{2,}\", \" \", text)\n\n    # 4. G·ªôp nhi·ªÅu space li√™n ti·∫øp\n    text = re.sub(r\"\\s+\", \" \", text)\n    \n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.37457Z","iopub.execute_input":"2025-12-13T14:13:52.37488Z","iopub.status.idle":"2025-12-13T14:13:52.388692Z","shell.execute_reply.started":"2025-12-13T14:13:52.374863Z","shell.execute_reply":"2025-12-13T14:13:52.388107Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# ƒê·ªçc d·ªØ li·ªáu Song ng·ªØ\nH√†m ƒë·ªçc ƒë·ªìng th·ªùi hai file ngu·ªìn (Ti·∫øng Anh) v√† ƒë√≠ch (Ti·∫øng Vi·ªát), gh√©p ch√∫ng th√†nh c√°c c·∫∑p c√¢u song song. C√°c c·∫∑p c√¢u b·ªã r·ªóng ho·∫∑c l·ªói s·∫Ω b·ªã lo·∫°i b·ªè ƒë·ªÉ kh√¥ng l√†m nhi·ªÖu qu√° tr√¨nh hu·∫•n luy·ªán.","metadata":{}},{"cell_type":"code","source":"# --- CELL 2: DATA READING FUNCTION ---\ndef read_parallel_files(src_filename, tgt_filename):\n    \"\"\"ƒê·ªçc c·∫∑p file song ng·ªØ, tr·∫£ v·ªÅ list c√°c tuple (c√¢u_ngu·ªìn, c√¢u_ƒë√≠ch)\"\"\"\n    # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n (h·ªó tr·ª£ c·∫£ th∆∞ m·ª•c hi·ªán t·∫°i v√† th∆∞ m·ª•c input c·ªßa Kaggle)\n    possible_paths = [\"./\", \"/kaggle/input/\", \"/kaggle/working/\"]\n    \n    src_path, tgt_path = None, None\n    for p in possible_paths:\n        if os.path.exists(os.path.join(p, src_filename)):\n            src_path = os.path.join(p, src_filename)\n        if os.path.exists(os.path.join(p, tgt_filename)):\n            tgt_path = os.path.join(p, tgt_filename)\n            \n    if not src_path or not tgt_path:\n        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {src_filename} ho·∫∑c {tgt_filename}. B·ªè qua.\")\n        return []\n\n    print(f\"üìñ ƒêang ƒë·ªçc: {src_path} v√† {tgt_path}\")\n    with open(src_path, 'r', encoding='utf-8') as f_src, \\\n         open(tgt_path, 'r', encoding='utf-8') as f_tgt:\n        src_lines = [clean_text(line.strip()) for line in f_src.read().splitlines()]\n        tgt_lines = [clean_text(line.strip()) for line in f_tgt.read().splitlines()]\n    \n    # L·ªçc b·ªè c√°c c·∫∑p c√¢u r·ªóng ho·∫∑c l·ªách d√≤ng\n    pairs = []\n    min_len = min(len(src_lines), len(tgt_lines))\n    for i in range(min_len):\n        if src_lines[i] and tgt_lines[i]:\n            pairs.append((src_lines[i], tgt_lines[i]))\n            \n    return pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.389517Z","iopub.execute_input":"2025-12-13T14:13:52.389775Z","iopub.status.idle":"2025-12-13T14:13:52.402777Z","shell.execute_reply.started":"2025-12-13T14:13:52.389753Z","shell.execute_reply":"2025-12-13T14:13:52.402037Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# T·∫£i v√† Chia t·∫≠p d·ªØ li·ªáu\nLoad 3 t·∫≠p d·ªØ li·ªáu chu·∫©n t·ª´ b·ªô NLP VLSP:\n- **Train set:** D√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.\n- **Validation set (tst2012):** D√πng ƒë·ªÉ tinh ch·ªânh v√† ki·ªÉm tra overfitting trong l√∫c train.\n- **Test set (tst2013):** D√πng ƒë·ªÉ ƒë√°nh gi√° ƒë·ªôc l·∫≠p sau c√πng.","metadata":{}},{"cell_type":"code","source":"# --- CELL 3: LOAD DATA ---\nprint(\"\\n--- ƒêANG T·∫¢I D·ªÆ LI·ªÜU ---\")\n# ƒê·∫£m b·∫£o t√™n file kh·ªõp v·ªõi file b·∫°n upload\ntrain_pairs = read_parallel_files(\"/kaggle/input/entovinlp/train.en\", \"/kaggle/input/entovinlp/train.vi\")\nval_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2012.en\", \"/kaggle/input/entovinlp/tst2012.vi\")\ntest_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2013.en\", \"/kaggle/input/entovinlp/tst2013.vi\")\n\nprint(f\"‚úÖ Train size: {len(train_pairs)}\")\nprint(f\"‚úÖ Val size: {len(val_pairs)}\")\nprint(f\"‚úÖ Test size: {len(test_pairs)}\")\n\nif len(train_pairs) > 0:\n    print(f\"üîé V√≠ d·ª• m·∫´u: {train_pairs[0:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.40469Z","iopub.execute_input":"2025-12-13T14:13:52.404954Z","iopub.status.idle":"2025-12-13T14:14:51.236442Z","shell.execute_reply.started":"2025-12-13T14:13:52.404939Z","shell.execute_reply":"2025-12-13T14:14:51.235694Z"}},"outputs":[{"name":"stdout","text":"\n--- ƒêANG T·∫¢I D·ªÆ LI·ªÜU ---\nüìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/train.en v√† /kaggle/input/entovinlp/train.vi\nüìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/tst2012.en v√† /kaggle/input/entovinlp/tst2012.vi\nüìñ ƒêang ƒë·ªçc: /kaggle/input/entovinlp/tst2013.en v√† /kaggle/input/entovinlp/tst2013.vi\n‚úÖ Train size: 133166\n‚úÖ Val size: 1553\n‚úÖ Test size: 1268\nüîé V√≠ d·ª• m·∫´u: [('rachel pike : the science behind a climate headline', 'khoa h·ªçc ƒë·∫±ng sau m·ªôt ti√™u ƒë·ªÅ v·ªÅ kh√≠ h·∫≠u'), ('in 4 minutes , atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team one of thousands who contributed taking a risky flight over the rainforest in pursuit of data on a key molecule .', 'trong 4 ph√∫t , chuy√™n gia ho√° h·ªçc kh√≠ quy·ªÉn rachel pike gi·ªõi thi·ªáu s∆° l∆∞·ª£c v·ªÅ nh·ªØng n·ªó l·ª±c khoa h·ªçc mi·ªát m√†i ƒë·∫±ng sau nh·ªØng ti√™u ƒë·ªÅ t√°o b·∫°o v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , c√πng v·ªõi ƒëo√†n nghi√™n c·ª©u c·ªßa m√¨nh h√†ng ng√†n ng∆∞·ªùi ƒë√£ c·ªëng hi·∫øn cho d·ª± √°n n√†y m·ªôt chuy·∫øn bay m·∫°o hi·ªÉm qua r·ª´ng gi√† ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin v·ªÅ m·ªôt ph√¢n t·ª≠ then ch·ªët .'), (\"i 'd like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\", 't√¥i mu·ªën cho c√°c b·∫°n bi·∫øt v·ªÅ s·ª± to l·ªõn c·ªßa nh·ªØng n·ªó l·ª±c khoa h·ªçc ƒë√£ g√≥p ph·∫ßn l√†m n√™n c√°c d√≤ng t√≠t b·∫°n th∆∞·ªùng th·∫•y tr√™n b√°o .'), ('headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .', 'c√≥ nh·ªØng d√≤ng tr√¥ng nh∆∞ th·∫ø n√†y khi b√†n v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , v√† nh∆∞ th·∫ø n√†y khi n√≥i v·ªÅ ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ hay kh√≥i b·ª•i .'), ('they are both two branches of the same field of atmospheric science .', 'c·∫£ hai ƒë·ªÅu l√† m·ªôt nh√°nh c·ªßa c√πng m·ªôt lƒ©nh v·ª±c trong ng√†nh khoa h·ªçc kh√≠ quy·ªÉn .')]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Hu·∫•n luy·ªán BPE Tokenizer\nS·ª≠ d·ª•ng thu·∫≠t to√°n **Byte Pair Encoding (BPE)** ƒë·ªÉ m√£ h√≥a vƒÉn b·∫£n.\n- Gi√∫p gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ t·ª´ v·ª±ng m·ªü (Out-of-Vocabulary) b·∫±ng c√°ch chia nh·ªè t·ª´ ch∆∞a bi·∫øt th√†nh c√°c subword.\n- K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn (Vocab Size): 10,000 token cho m·ªói ng√¥n ng·ªØ.","metadata":{}},{"cell_type":"code","source":"# --- CELL 4: TRAIN TOKENIZERS ---\nprint(\"\\n--- HU·∫§N LUY·ªÜN TOKENIZER ---\")\n\ndef train_bpe_tokenizer(texts, vocab_size=8000):\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n    tokenizer.decoder = decoders.ByteLevel()\n    \n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[UNK]\"],\n        show_progress=False\n    )\n    tokenizer.train_from_iterator(texts, trainer=trainer)\n    return tokenizer\n\n# G·ªôp text ƒë·ªÉ train tokenizer\nall_src_text = [p[0] for p in train_pairs + val_pairs]\nall_tgt_text = [p[1] for p in train_pairs + val_pairs]\n\nif not all_src_text: # Dummy data n·∫øu ch∆∞a load ƒë∆∞·ª£c file\n    all_src_text = [\"Hello world\"]\n    all_tgt_text = [\"Xin ch√†o\"]\n\nen_tokenizer = train_bpe_tokenizer(all_src_text, vocab_size=10000)\nvi_tokenizer = train_bpe_tokenizer(all_tgt_text, vocab_size=10000)\n\n# L·∫•y ID c√°c token ƒë·∫∑c bi·ªát\nPAD_ID = en_tokenizer.token_to_id(\"[PAD]\")\nSTART_ID = vi_tokenizer.token_to_id(\"[START]\")\nEND_ID = vi_tokenizer.token_to_id(\"[END]\")\n\nprint(\"‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:51.23734Z","iopub.execute_input":"2025-12-13T14:14:51.237712Z","iopub.status.idle":"2025-12-13T14:14:57.310542Z","shell.execute_reply.started":"2025-12-13T14:14:51.237683Z","shell.execute_reply":"2025-12-13T14:14:57.309854Z"}},"outputs":[{"name":"stdout","text":"\n--- HU·∫§N LUY·ªÜN TOKENIZER ---\n‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# X√¢y d·ª±ng Dataset & Batching\n- **EnViDataset:** Class ch·ªãu tr√°ch nhi·ªám l∆∞u tr·ªØ v√† truy xu·∫•t t·ª´ng c·∫∑p c√¢u.\n- **Collate Function:** H√†m x·ª≠ l√Ω t·ª´ng batch d·ªØ li·ªáu tr∆∞·ªõc khi ƒë∆∞a v√†o GPU:\n  - M√£ h√≥a text sang ID s·ªë.\n  - Th√™m token ƒë·∫∑c bi·ªát `[START]` v√† `[END]` cho c√¢u ƒë√≠ch.\n  - **Padding:** Th√™m token `[PAD]` ƒë·ªÉ ƒë·ªô d√†i c√°c c√¢u trong m·ªôt batch b·∫±ng nhau.","metadata":{}},{"cell_type":"code","source":"# --- CELL 5: DATASET CLASS ---\nclass EnViDataset(Dataset):\n    def __init__(self, pairs):\n        self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        return self.pairs[idx]\n\ndef collate_fn(batch):\n    en_batch, vi_batch = zip(*batch)\n    \n    # Encode ti·∫øng Anh (Source)\n    en_enc = en_tokenizer.encode_batch(list(en_batch))\n    en_ids = [e.ids for e in en_enc]\n    \n    # Encode ti·∫øng Vi·ªát (Target) - Th√™m START v√† END th·ªß c√¥ng\n    vi_ids = []\n    for text in vi_batch:\n        ids = vi_tokenizer.encode(text).ids\n        vi_ids.append([START_ID] + ids + [END_ID])\n    \n    # Padding\n    max_len_en = max([len(x) for x in en_ids])\n    max_len_vi = max([len(x) for x in vi_ids])\n    \n    padded_en = [x + [PAD_ID] * (max_len_en - len(x)) for x in en_ids]\n    padded_vi = [x + [PAD_ID] * (max_len_vi - len(x)) for x in vi_ids]\n    \n    return torch.tensor(padded_en), torch.tensor(padded_vi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.31126Z","iopub.execute_input":"2025-12-13T14:14:57.311593Z","iopub.status.idle":"2025-12-13T14:14:57.317842Z","shell.execute_reply.started":"2025-12-13T14:14:57.311573Z","shell.execute_reply":"2025-12-13T14:14:57.317178Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Kh·ªüi t·∫°o DataLoader\nT·∫°o c√°c iterator ƒë·ªÉ n·∫°p d·ªØ li·ªáu v√†o m√¥ h√¨nh theo t·ª´ng Batch (k√≠ch th∆∞·ªõc 32).\n* `shuffle=True` cho t·∫≠p Train ƒë·ªÉ x√°o tr·ªôn d·ªØ li·ªáu, gi√∫p m√¥ h√¨nh h·ªçc t·ªïng qu√°t h∆°n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 6: DATALOADERS ---\nBATCH_SIZE = 32\n\ntrain_loader = DataLoader(EnViDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(EnViDataset(val_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\nprint(f\"DataLoaders created. Batch size: {BATCH_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.318602Z","iopub.execute_input":"2025-12-13T14:14:57.318812Z","iopub.status.idle":"2025-12-13T14:14:57.332413Z","shell.execute_reply.started":"2025-12-13T14:14:57.318786Z","shell.execute_reply":"2025-12-13T14:14:57.331847Z"}},"outputs":[{"name":"stdout","text":"DataLoaders created. Batch size: 32\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# C√°c th√†nh ph·∫ßn Ki·∫øn tr√∫c N√¢ng cao (Advanced Architecture)\nNh√≥m s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t hi·ªán ƒë·∫°i (t∆∞∆°ng t·ª± LLaMA) thay v√¨ Transformer c·ªï ƒëi·ªÉn:\n1. **Rotary Positional Embeddings (RoPE):** M√£ h√≥a v·ªã tr√≠ d·∫°ng xoay, gi√∫p m√¥ h√¨nh n·∫Øm b·∫Øt v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi t·ªët h∆°n Sinusoidal truy·ªÅn th·ªëng.\n2. **SwiGLU Activation:** H√†m k√≠ch ho·∫°t thay th·∫ø ReLU, gi√∫p lu·ªìng gradient ·ªïn ƒë·ªãnh v√† h·ªôi t·ª• t·ªët h∆°n.\n3. **Grouped Query Attention (GQA):** T·ªëi ∆∞u h√≥a Multi-head Attention b·∫±ng c√°ch chia s·∫ª Key/Value heads, gi·∫£m chi ph√≠ t√≠nh to√°n m√† v·∫´n gi·ªØ ƒë·ªô ch√≠nh x√°c.","metadata":{}},{"cell_type":"code","source":"# --- CELL 7: MODEL COMPONENTS ---\n# --- Rotary Positional Embeddings ---\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(x, cos, sin):\n    return (x * cos) + (rotate_half(x) * sin)\n\nclass RotaryPositionalEncoding(nn.Module):\n    def __init__(self, head_dim, max_seq_len=2048):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n        t = torch.arange(max_seq_len).float()\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos\", emb.cos()[None, :, None, :])\n        self.register_buffer(\"sin\", emb.sin()[None, :, None, :])\n\n    def forward(self, x, seq_len):\n        return self.cos[:, :seq_len, :, :], self.sin[:, :seq_len, :, :]\n\n# --- Feed Forward (SwiGLU) ---\nclass SwiGLU(nn.Module):\n    def __init__(self, hidden_dim, intermediate_dim):\n        super().__init__()\n        self.w1 = nn.Linear(hidden_dim, intermediate_dim)\n        self.w2 = nn.Linear(hidden_dim, intermediate_dim)\n        self.w3 = nn.Linear(intermediate_dim, hidden_dim)\n\n    def forward(self, x):\n        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n\n# --- GQA Attention ---\nclass GQA(nn.Module):\n    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_dim = hidden_dim // num_heads\n        self.num_groups = num_heads // num_kv_heads\n        \n        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.k_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n        self.v_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = dropout\n\n    def forward(self, x, enc_out=None, mask=None, rope_cos=None, rope_sin=None):\n        batch, seq_len, _ = x.shape\n        kv_input = enc_out if enc_out is not None else x\n        kv_seq_len = kv_input.shape[1]\n\n        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)\n        k = self.k_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n        v = self.v_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n\n        if rope_cos is not None and enc_out is None:\n            q = apply_rotary_pos_emb(q, rope_cos, rope_sin)\n            k = apply_rotary_pos_emb(k, rope_cos, rope_sin)\n\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if self.num_groups > 1:\n            k = k[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n            v = v[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n\n        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0)\n        return self.o_proj(out.transpose(1, 2).reshape(batch, seq_len, -1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.333208Z","iopub.execute_input":"2025-12-13T14:14:57.333937Z","iopub.status.idle":"2025-12-13T14:14:57.352787Z","shell.execute_reply.started":"2025-12-13T14:14:57.333914Z","shell.execute_reply":"2025-12-13T14:14:57.35212Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# L·∫Øp r√°p M√¥ h√¨nh Transformer\nK·∫øt h·ª£p c√°c th√†nh ph·∫ßn tr√™n th√†nh ki·∫øn tr√∫c Encoder-Decoder ho√†n ch·ªânh:\n* **Encoder:** M√£ h√≥a c√¢u ti·∫øng Anh th√†nh vector ng·ªØ nghƒ©a.\n* **Decoder:** Sinh c√¢u ti·∫øng Vi·ªát t·ª´ vector ng·ªØ nghƒ©a, s·ª≠ d·ª•ng c∆° ch·∫ø Cross-Attention ƒë·ªÉ \"nh√¨n\" l·∫°i c√¢u ngu·ªìn.","metadata":{}},{"cell_type":"code","source":"# --- CELL 8: TRANSFORMER MODEL ---\nclass TransformerBlock(nn.Module):\n    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1, is_decoder=False):\n        super().__init__()\n        self.norm1 = nn.RMSNorm(hidden_dim)\n        self.attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n        self.is_decoder = is_decoder\n        if is_decoder:\n            self.norm2 = nn.RMSNorm(hidden_dim)\n            self.cross_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n        self.norm_ffn = nn.RMSNorm(hidden_dim)\n        self.ffn = SwiGLU(hidden_dim, hidden_dim * 4)\n\n    def forward(self, x, enc_out=None, mask=None, cross_mask=None, rope_cos=None, rope_sin=None):\n        x = x + self.attn(self.norm1(x), mask=mask, rope_cos=rope_cos, rope_sin=rope_sin)\n        if self.is_decoder:\n            x = x + self.cross_attn(self.norm2(x), enc_out=enc_out, mask=cross_mask)\n        x = x + self.ffn(self.norm_ffn(x))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4):\n        super().__init__()\n        self.src_emb = nn.Embedding(src_vocab, hidden_dim)\n        self.tgt_emb = nn.Embedding(tgt_vocab, hidden_dim)\n        self.rope = RotaryPositionalEncoding(hidden_dim // num_heads)\n        self.encoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads) for _ in range(num_layers)])\n        self.decoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads, is_decoder=True) for _ in range(num_layers)])\n        self.final_norm = nn.RMSNorm(hidden_dim)\n        self.fc_out = nn.Linear(hidden_dim, tgt_vocab)\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        x = self.src_emb(src)\n        rope_cos, rope_sin = self.rope(x, x.shape[1])\n        for layer in self.encoders:\n            x = layer(x, mask=src_mask, rope_cos=rope_cos, rope_sin=rope_sin)\n        enc_out = x\n        \n        x = self.tgt_emb(tgt)\n        rope_cos_tgt, rope_sin_tgt = self.rope(x, x.shape[1])\n        for layer in self.decoders:\n            x = layer(x, enc_out=enc_out, mask=tgt_mask, cross_mask=src_mask, rope_cos=rope_cos_tgt, rope_sin=rope_sin_tgt)\n        return self.fc_out(self.final_norm(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.353411Z","iopub.execute_input":"2025-12-13T14:14:57.353665Z","iopub.status.idle":"2025-12-13T14:14:57.370367Z","shell.execute_reply.started":"2025-12-13T14:14:57.353643Z","shell.execute_reply":"2025-12-13T14:14:57.36971Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Thi·∫øt l·∫≠p Hu·∫•n luy·ªán\n- **Masking:** T·∫°o che (mask) ƒë·ªÉ ƒë·∫£m b·∫£o Decoder kh√¥ng \"nh√¨n th·∫•y\" t∆∞∆°ng lai khi d·ª± ƒëo√°n t·ª´ ti·∫øp theo.\n- **Label Smoothing (0.1):** K·ªπ thu·∫≠t Regularization gi√∫p m√¥ h√¨nh b·ªõt t·ª± tin th√°i qu√° v√†o d·ªØ li·ªáu train, gi·∫£m Overfitting.\n- **Optimizer:** AdamW (bi·∫øn th·ªÉ c·ªßa Adam v·ªõi Weight Decay t√°ch bi·ªát) gi√∫p t·ªëi ∆∞u t·ªët h∆°n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 9: INIT TRAINING ---\ndef create_masks(src, tgt):\n    src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n    batch, seq_len = tgt.shape\n    causal = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n    tgt_pad = (tgt == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n    return src_mask, causal + tgt_pad\n\nmodel = Transformer(\n    src_vocab=en_tokenizer.get_vocab_size(),\n    tgt_vocab=vi_tokenizer.get_vocab_size(),\n    hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n\nprint(\"Model Initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.371004Z","iopub.execute_input":"2025-12-13T14:14:57.371268Z","iopub.status.idle":"2025-12-13T14:15:00.252982Z","shell.execute_reply.started":"2025-12-13T14:14:57.371205Z","shell.execute_reply":"2025-12-13T14:15:00.25222Z"}},"outputs":[{"name":"stdout","text":"Model Initialized.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Qu√° tr√¨nh Hu·∫•n luy·ªán (Training Loop)\nCh·∫°y hu·∫•n luy·ªán qua 10 Epochs. T·∫°i m·ªói Epoch:\n* T√≠nh Loss tr√™n t·∫≠p Train.\n* ƒê√°nh gi√° ngay tr√™n t·∫≠p Validation ƒë·ªÉ theo d√µi s·ª± h·ªôi t·ª•.\n* L∆∞u l·∫°i Model sau khi ho√†n t·∫•t.","metadata":{}},{"cell_type":"code","source":"# --- CELL 10: TRAINING LOOP ---\nEPOCHS = 10\nprint(\"\\n--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ---\")\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for src, tgt in pbar:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n        src_mask, tgt_mask = create_masks(src, tgt_input)\n        \n        optimizer.zero_grad()\n        output = model(src, tgt_input, src_mask, tgt_mask)\n        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        \n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for src, tgt in val_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n            src_mask, tgt_mask = create_masks(src, tgt_input)\n            output = model(src, tgt_input, src_mask, tgt_mask)\n            val_loss += criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1)).item()\n            \n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n\ntorch.save(model.state_dict(), \"transformer_en_vi.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:15:00.253837Z","iopub.execute_input":"2025-12-13T14:15:00.254194Z","iopub.status.idle":"2025-12-13T15:27:57.272211Z","shell.execute_reply.started":"2025-12-13T14:15:00.254162Z","shell.execute_reply":"2025-12-13T15:27:57.271409Z"}},"outputs":[{"name":"stdout","text":"\n--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:13<00:00,  9.59it/s, loss=3.7543]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 4.3289 | Val Loss: 3.7808\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:17<00:00,  9.52it/s, loss=3.3652]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 3.4575 | Val Loss: 3.5218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:17<00:00,  9.51it/s, loss=3.0487]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 3.1958 | Val Loss: 3.4255\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:15<00:00,  9.56it/s, loss=2.9763]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 3.0395 | Val Loss: 3.3636\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:16<00:00,  9.54it/s, loss=2.9370]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 2.9232 | Val Loss: 3.3323\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:16<00:00,  9.54it/s, loss=2.8674]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 2.8282 | Val Loss: 3.3241\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:17<00:00,  9.52it/s, loss=2.6695]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 2.7478 | Val Loss: 3.3162\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:16<00:00,  9.54it/s, loss=2.4906]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 2.6763 | Val Loss: 3.3267\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:16<00:00,  9.52it/s, loss=2.5666]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 2.6129 | Val Loss: 3.3363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4162/4162 [07:17<00:00,  9.51it/s, loss=2.7093]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 2.5556 | Val Loss: 3.3555\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Ki·ªÉm th·ª≠ (Inference)\nH√†m d·ªãch s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p **Greedy Decoding**: T·∫°i m·ªói b∆∞·ªõc, ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t l√†m t·ª´ ti·∫øp theo cho b·∫£n d·ªãch.","metadata":{}},{"cell_type":"code","source":"# --- CELL 11: INFERENCE ---\nprint(\"\\n--- TEST K·∫æT QU·∫¢ ---\")\ndef translate(sentence, max_len=100):\n    model.eval()\n    with torch.no_grad():\n        src = torch.tensor([en_tokenizer.encode(sentence).ids]).to(device)\n        src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n        tgt_ids = [START_ID]\n        \n        for _ in range(max_len):\n            tgt = torch.tensor([tgt_ids]).to(device)\n            causal = torch.triu(torch.full((tgt.shape[1], tgt.shape[1]), float('-inf'), device=device), diagonal=1)\n            out = model(src, tgt, src_mask, causal)\n            next_token = out[0, -1, :].argmax().item()\n            if next_token == END_ID: break\n            tgt_ids.append(next_token)\n        return vi_tokenizer.decode(tgt_ids[1:])\n\nfor i in range(5):\n    if len(test_pairs) > 0:\n        idx = random.randint(0, len(test_pairs)-1)\n        en_txt, vi_txt = test_pairs[idx]\n        print(f\"üîπ Input:  {en_txt}\\nüî∏ Target: {vi_txt}\\nüöÄ Model:  {translate(en_txt)}\\n{'-'*50}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:27:57.273337Z","iopub.execute_input":"2025-12-13T15:27:57.27408Z","iopub.status.idle":"2025-12-13T15:27:58.35176Z","shell.execute_reply.started":"2025-12-13T15:27:57.27406Z","shell.execute_reply":"2025-12-13T15:27:58.351153Z"}},"outputs":[{"name":"stdout","text":"\n--- TEST K·∫æT QU·∫¢ ---\nüîπ Input:  and he said that he needed those guns because of the trauma he 'd experienced as a young boy .\nüî∏ Target: v√† anh ta n√≥i r·∫±ng anh ta c·∫ßn nh·ªØng c√¢y s√∫ng n√†y b·ªüi v√¨ nh·ªØng t·ªïn th∆∞∆°ng m√† anh ƒë√£ tr·∫£i qua trong qu√° kh·ª© khi l√† m·ªôt ƒë·ª©a tr·∫ª .\nüöÄ Model:   v√† √¥ng n√≥i r·∫±ng √¥ng c·∫ßn nh·ªØng ng∆∞·ªùi s√∫ng ƒë√≥ v√¨ nh·ªØng ch·∫•n ƒë·ªông ƒë√≥ √¥ng ƒë√£ tr·∫£i qua khi c√≤n l√† m·ªôt c·∫≠u b√© .\n--------------------------------------------------\nüîπ Input:  am i south korean or north korean ?\nüî∏ Target: t√¥i l√† ng∆∞·ªùi nam tri·ªÅu ti√™n hay b·∫Øc tri·ªÅu ti√™n ?\nüöÄ Model:   t√¥i c√≥ ph·∫£i l√† ng∆∞·ªùi h√†n qu·ªëc hay b·∫Øc tri·ªÅu ti√™n hay b·∫Øc tri·ªÅu ti√™n ?\n--------------------------------------------------\nüîπ Input:  and so just as the womb entirely envelopes the embryo , which grows within it , the divine matrix of compassion nourishes the entire existence .\nüî∏ Target: v√† b·ªüi v√¨ t·ª≠ cung bao b·ªçc ho√†n to√†n ph√¥i thai ƒëang ph√°t tri·ªÉn trong l√≤ng n√≥ , ma tr·∫≠n thi√™ng li√™ng c·ªßa t√¨nh th∆∞∆°ng nu√¥i d∆∞·ª°ng to√†n b·ªô s·ª± s·ªëng ƒë√≥ .\nüöÄ Model:   v√† nh∆∞ nh·ªØng ƒë·ªông v·∫≠t ƒë∆∞·ª£c bao g·ªìm c·∫£ ph√¥i thai , ƒë∆∞·ª£c ph√°t tri·ªÉn b√™n trong n√≥ , ma tr·∫≠n c·ªßa s·ª± nh√¢n √°i c·ªßa l√≤ng t·ª´ bi v√† s·ª± t·ªìn t·∫°i c·ªßa t·∫•t c·∫£ s·ª± t·ªìn t·∫°i .\n--------------------------------------------------\nüîπ Input:  it 's really become sacred to us .\nüî∏ Target: n√≥ tr·ªü n√™n th·∫≠t thi√™ng li√™ng v·ªõi ch√∫ng t√¥i .\nüöÄ Model:   n√≥ th·ª±c s·ª± tr·ªü th√†nh thi√™ng li√™ng c·ªßa ch√∫ng ta .\n--------------------------------------------------\nüîπ Input:  this is a visualization of six months of my life .\nüî∏ Target: ƒë√¢y l√† nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ cu·ªôc s·ªëng trong s√°u th√°ng ƒë√£ ƒë∆∞·ª£c ghi l·∫°i c·ªßa t√¥i .\nüöÄ Model:   ƒë√¢y l√† h√¨nh ·∫£nh c·ªßa 6 th√°ng c·ªßa ƒë·ªùi t√¥i .\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng (Metric Evaluation)\nS·ª≠ d·ª•ng **BLEU Score** (th√¥ng qua th∆∞ vi·ªán `sacrebleu`) - ti√™u chu·∫©n v√†ng trong ƒë√°nh gi√° d·ªãch m√°y.\n * ƒêi·ªÉm BLEU ƒë∆∞·ª£c t√≠nh tr√™n t·∫≠p Test (ho√†n to√†n m·ªõi v·ªõi m√¥ h√¨nh).\n * K·∫øt qu·∫£ ph·∫£n √°nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa vƒÉn b·∫£n m√°y d·ªãch v√† vƒÉn b·∫£n m·∫´u c·ªßa con ng∆∞·ªùi.","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:27:58.442004Z","iopub.execute_input":"2025-12-13T15:27:58.442513Z","iopub.status.idle":"2025-12-13T15:28:02.798777Z","shell.execute_reply.started":"2025-12-13T15:27:58.442495Z","shell.execute_reply":"2025-12-13T15:28:02.797767Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import sacrebleu\nimport random\nfrom tqdm import tqdm # Thanh hi·ªÉn th·ªã ti·∫øn ƒë·ªô\n\ndef calculate_bleu(data_pairs, num_samples=100):\n    print(f\"--- üìä ƒêANG T√çNH ƒêI·ªÇM BLEU TR√äN {num_samples} M·∫™U ---\")\n    \n    # Ch·ªçn ng·∫´u nhi√™n m·∫´u ƒë·ªÉ test (ho·∫∑c l·∫•y h·∫øt n·∫øu num_samples=None)\n    if num_samples is not None and num_samples < len(data_pairs):\n        samples = random.sample(data_pairs, num_samples)\n    else:\n        samples = data_pairs\n\n    preds = [] # C√°c c√¢u m√°y d·ªãch\n    refs = []  # C√°c c√¢u ƒë√°p √°n chu·∫©n\n\n    # B·∫Øt ƒë·∫ßu d·ªãch\n    for en_txt, vi_txt in tqdm(samples):\n        # D·ªãch c√¢u ti·∫øng Anh\n        pred_sent = translate(en_txt)\n        \n        preds.append(pred_sent)\n        refs.append(vi_txt) # Sacrebleu nh·∫≠n list c√°c string cho refs\n\n    # T√≠nh ƒëi·ªÉm BLEU\n    # refs c·∫ßn ƒë∆∞·ª£c b·ªçc trong list v√¨ 1 c√¢u input c√≥ th·ªÉ c√≥ nhi·ªÅu c√¢u target (·ªü ƒë√¢y ta c√≥ 1)\n    bleu = sacrebleu.corpus_bleu(preds, [refs])\n    \n    return bleu.score\n\n# --- CH·∫†Y T√çNH ƒêI·ªÇM ---\n# B·∫°n c√≥ th·ªÉ tƒÉng s·ªë l∆∞·ª£ng m·∫´u l√™n len(test_pairs) ƒë·ªÉ ch√≠nh x√°c h∆°n (s·∫Ω ch·∫°y l√¢u h∆°n)\nscore = calculate_bleu(test_pairs, num_samples= None)\n\nprint(f\"\\nüåü ƒêI·ªÇM BLEU C·ª¶A MODEL: {score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:40:04.758742Z","iopub.execute_input":"2025-12-13T15:40:04.759044Z","iopub.status.idle":"2025-12-13T15:45:36.947251Z","shell.execute_reply.started":"2025-12-13T15:40:04.759024Z","shell.execute_reply":"2025-12-13T15:45:36.946704Z"}},"outputs":[{"name":"stdout","text":"--- üìä ƒêANG T√çNH ƒêI·ªÇM BLEU TR√äN None M·∫™U ---\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1268/1268 [05:31<00:00,  3.82it/s]\nThat's 100 lines that end in a tokenized period ('.')\nIt looks like you forgot to detokenize your test data, which may hurt your score.\nIf you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\nüåü ƒêI·ªÇM BLEU C·ª¶A MODEL: 27.83\n","output_type":"stream"}],"execution_count":17}]}