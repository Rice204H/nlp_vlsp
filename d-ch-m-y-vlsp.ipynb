{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14093038,"sourceType":"datasetVersion","datasetId":8974101},{"sourceId":14120557,"sourceType":"datasetVersion","datasetId":8995850}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/luuvnhng/d-ch-m-y-transformer?scriptVersionId=285945892\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# XÃ¢y dá»±ng MÃ´ hÃ¬nh Dá»‹ch mÃ¡y báº±ng Transformer","metadata":{}},{"cell_type":"markdown","source":"# Chuáº©n bá»‹ dá»¯ liá»‡u\n\nVÃ­ dá»¥ thá»±c nghiá»‡m dá»±a trÃªn cáº·p dá»¯ liá»‡u Anh-Viá»‡t nguá»“n tá»« iwslt vá»›i 133k cáº·p cÃ¢u:\n\n*cd data/iwslt_en_vi*\n\nDá»¯ liá»‡u bao gá»“m cÃ¢u nguá»“n (src) vÃ  cÃ¢u Ä‘Ã­ch (tgt) dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tÃ¡ch tá»«:\n\n* train.en\n* train.vi\n* tst2012.en\n* tst2012.vi\n* tst2013.en\n* tst2013.vi\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:48.81309Z","iopub.execute_input":"2025-12-13T14:13:48.813373Z","iopub.status.idle":"2025-12-13T14:13:49.085505Z","shell.execute_reply.started":"2025-12-13T14:13:48.813351Z","shell.execute_reply":"2025-12-13T14:13:49.084919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thiáº¿t láº­p MÃ´i trÆ°á»ng & ThÆ° viá»‡n\nKhá»Ÿi táº¡o cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t (Torch, Tokenizers) vÃ  thiáº¿t láº­p `SEED` cá»‘ Ä‘á»‹nh Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ cÃ³ thá»ƒ tÃ¡i láº­p (reproducible) trong cÃ¡c láº§n cháº¡y khÃ¡c nhau.\nKiá»ƒm tra GPU: Sá»­ dá»¥ng CUDA náº¿u cÃ³ Ä‘á»ƒ tÄƒng tá»‘c huáº¥n luyá»‡n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 1: SETUP ---\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport tqdm\nimport warnings\n\n# CÃ i Ä‘áº·t thÆ° viá»‡n tokenizers náº¿u chÆ°a cÃ³\ntry:\n    import tokenizers\nexcept ImportError:\n    os.system('pip install tokenizers')\n    import tokenizers\n\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n\n# Cáº¥u hÃ¬nh thiáº¿t bá»‹ vÃ  Random Seed\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ”¹ Äang sá»­ dá»¥ng thiáº¿t bá»‹: {device}\")\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:49.086796Z","iopub.execute_input":"2025-12-13T14:13:49.087074Z","iopub.status.idle":"2025-12-13T14:13:52.373875Z","shell.execute_reply.started":"2025-12-13T14:13:49.087053Z","shell.execute_reply":"2025-12-13T14:13:52.373202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u (Data Preprocessing)\nHÃ m `clean_text` thá»±c hiá»‡n chuáº©n hÃ³a dá»¯ liá»‡u vÄƒn báº£n thÃ´:\n- **Unicode Normalization:** Chuyá»ƒn vá» chuáº©n NFC (quan trá»ng cho tiáº¿ng Viá»‡t).\n- **Lowercase:** Chuyá»ƒn vá» chá»¯ thÆ°á»ng Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn.\n- **Contraction Expansion:** TÃ¡ch cÃ¡c tá»« viáº¿t táº¯t tiáº¿ng Anh (vÃ­ dá»¥: \"I'm\" -> \"I am\", \"don't\" -> \"do not\") Ä‘á»ƒ mÃ´ hÃ¬nh há»c ngá»¯ phÃ¡p tá»‘t hÆ¡n.\n- **HTML Decoding:** Xá»­ lÃ½ cÃ¡c kÃ½ tá»± mÃ£ hÃ³a HTML.","metadata":{}},{"cell_type":"code","source":"import re\nimport html\nimport unicodedata\n\"\"\"\n    HÃ m lÃ m sáº¡ch cÆ¡ báº£n, dÃ¹ng CHUNG cho cáº£ train.vi, train.en vÃ  cÃ¢u input khi dá»‹ch.\n    Má»¥c tiÃªu:\n      - Bá» khoáº£ng tráº¯ng thá»«a, kÃ½ tá»± vÃ´ hÃ¬nh\n      - Giáº£i mÃ£ HTML (&quot; -> \")\n      - Chuáº©n hÃ³a Unicode NFC (dáº¥u tiáº¿ng Viá»‡t)\n      - ÄÆ°a vá» chá»¯ thÆ°á»ng (ráº¥t quan trá»ng vÃ¬ Field(lower=True))\n      - Chuáº©n hÃ³a dáº¥u cÃ¢u: tÃ¡ch , . ! ? ; : () \" ' ra khá»i tá»«\n    \"\"\"\ndef clean_text(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n\n    text = text.strip()\n    text = unicodedata.normalize(\"NFC\", text)\n    text = text.lower()\n    text = re.sub(r\"<[^>]+>\", \" \", text)\n    if not text:\n        return \"\"\n\n    # 1. Giáº£i mÃ£ html\n    text = html.unescape(text)\n\n    # 2. Expand contraction (nhÆ° anh Ä‘Ã£ viáº¿t)\n    split_patterns = [\n        # BE (am/is/are)\n        (r\"\\bi\\s*m\\b\", \"i am\"),\n        (r\"\\byou\\s*re\\b\", \"you are\"),\n        (r\"\\bwe\\s*re\\b\", \"we are\"),\n        (r\"\\bthey\\s*re\\b\", \"they are\"),\n        (r\"\\bhe\\s*s\\b\", \"he is\"),\n        (r\"\\bshe\\s*s\\b\", \"she is\"),\n        (r\"\\bit\\s*s\\b\", \"it is\"),\n        (r\"\\bthat\\s*s\\b\", \"that is\"),\n        (r\"\\bthere\\s*s\\b\", \"there is\"),\n        (r\"\\bhere\\s*s\\b\", \"here is\"),\n    \n        # HAVE\n        (r\"\\bi\\s*ve\\b\", \"i have\"),\n        (r\"\\byou\\s*ve\\b\", \"you have\"),\n        (r\"\\bwe\\s*ve\\b\", \"we have\"),\n        (r\"\\bthey\\s*ve\\b\", \"they have\"),\n    \n        # WILL\n        (r\"\\bi\\s*ll\\b\", \"i will\"),\n        (r\"\\byou\\s*ll\\b\", \"you will\"),\n        (r\"\\bwe\\s*ll\\b\", \"we will\"),\n        (r\"\\bthey\\s*ll\\b\", \"they will\"),\n        (r\"\\bhe\\s*ll\\b\", \"he will\"),\n        (r\"\\bshe\\s*ll\\b\", \"she will\"),\n        (r\"\\bit\\s*ll\\b\", \"it will\"),\n    \n        # WOULD\n        (r\"\\bi\\s*d\\b\", \"i would\"),\n        (r\"\\byou\\s*d\\b\", \"you would\"),\n        (r\"\\bwe\\s*d\\b\", \"we would\"),\n        (r\"\\bthey\\s*d\\b\", \"they would\"),\n        (r\"\\bhe\\s*d\\b\", \"he would\"),\n        (r\"\\bshe\\s*d\\b\", \"she would\"),\n        (r\"\\bit\\s*d\\b\", \"it would\"),\n        (r\"\\bthat\\s*d\\b\", \"that would\"),\n    \n        # NOT (full)\n        (r\"\\bdon\\s*t\\b\", \"do not\"),\n        (r\"\\bdoesn\\s*t\\b\", \"does not\"),\n        (r\"\\bdidn\\s*t\\b\", \"did not\"),\n        (r\"\\bcan\\s*t\\b\", \"can not\"),\n        (r\"\\bcouldn\\s*t\\b\", \"could not\"),\n        (r\"\\bshouldn\\s*t\\b\", \"should not\"),\n        (r\"\\bwouldn\\s*t\\b\", \"would not\"),\n        (r\"\\bisn\\s*t\\b\", \"is not\"),\n        (r\"\\baren\\s*t\\b\", \"are not\"),\n        (r\"\\bwasn\\s*t\\b\", \"was not\"),\n        (r\"\\bweren\\s*t\\b\", \"were not\"),\n        (r\"\\bwon\\s*t\\b\", \"will not\"),\n        (r\"\\bmustn\\s*t\\b\", \"must not\"),\n        (r\"\\bneedn\\s*t\\b\", \"need not\"),\n        (r\"\\bmightn\\s*t\\b\", \"might not\"),\n    \n        # OTHER\n        (r\"\\blet\\s*s\\b\", \"let us\"),\n    ]\n\n    for pat, repl in split_patterns:\n        text = re.sub(pat, repl, text)\n\n    full_patterns = [\n        # BE\n        (r\"\\bi m\\b\", \"i am\"),\n        (r\"\\byou re\\b\", \"you are\"),\n        (r\"\\bwe re\\b\", \"we are\"),\n        (r\"\\bthey re\\b\", \"they are\"),\n        (r\"\\bhe s\\b\", \"he is\"),\n        (r\"\\bshe s\\b\", \"she is\"),\n        (r\"\\bit s\\b\", \"it is\"),\n        (r\"\\bthat s\\b\", \"that is\"),\n        (r\"\\bhere s\\b\", \"here is\"),\n        (r\"\\bthere s\\b\", \"there is\"),\n    \n        # HAVE\n        (r\"\\bi ve\\b\", \"i have\"),\n        (r\"\\byou ve\\b\", \"you have\"),\n        (r\"\\bwe ve\\b\", \"we have\"),\n        (r\"\\bthey ve\\b\", \"they have\"),\n\n        # WILL\n        (r\"\\bi ll\\b\", \"I will\"),\n        (r\"\\byou ll\\b\", \"You will\"),\n        (r\"\\bwe ll\\b\", \"We will\"),\n        (r\"\\bthey ll\\b\", \"They will\"),\n        (r\"\\bhe ll\\b\", \"He will\"),\n        (r\"\\bshe ll\\b\", \"She will\"),\n        (r\"\\bit ll\\b\", \"It will\"),\n    \n        # WOULD\n        (r\"\\bi d\\b\", \"I would\"),\n        (r\"\\byou d\\b\", \"You would\"),\n        (r\"\\bwe d\\b\", \"We would\"),\n        (r\"\\bthey d\\b\", \"They would\"),\n        (r\"\\bhe d\\b\", \"He would\"),\n        (r\"\\bshe d\\b\", \"She would\"),\n        (r\"\\bit d\\b\", \"It would\"),\n        (r\"\\bthat d\\b\", \"That would\"),\n    \n        # NOT\n        (r\"\\bdon t\\b\", \"do not\"),\n        (r\"\\bdoesn t\\b\", \"does not\"),\n        (r\"\\bdidn t\\b\", \"did not\"),\n        (r\"\\bcan t\\b\", \"can not\"),\n        (r\"\\bcouldn t\\b\", \"could not\"),\n        (r\"\\bshouldn t\\b\", \"should not\"),\n        (r\"\\bwouldn t\\b\", \"would not\"),\n        (r\"\\bisn t\\b\", \"is not\"),\n        (r\"\\baren t\\b\", \"are not\"),\n        (r\"\\bwasn t\\b\", \"was not\"),\n        (r\"\\bweren t\\b\", \"were not\"),\n        (r\"\\bwon t\\b\", \"will not\"),\n        (r\"\\bmustn t\\b\", \"must not\"),\n        (r\"\\bneedn t\\b\", \"need not\"),\n        (r\"\\bmightn t\\b\", \"might not\"),\n    \n        # OTHER\n        (r\"\\blet s\\b\", \"let us\"),\n    ]\n\n    for pat, repl in full_patterns:\n        text = re.sub(pat, repl, text)\n\n    # 3. Bá» kÃ½ tá»± vÃ´ hÃ¬nh + bá» --, ---, ...\n    text = text.replace(\"\\u200b\", \"\")\n    text = re.sub(r\"-{2,}\", \" \", text)\n\n    # 4. Gá»™p nhiá»u space liÃªn tiáº¿p\n    text = re.sub(r\"\\s+\", \" \", text)\n    \n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.37457Z","iopub.execute_input":"2025-12-13T14:13:52.37488Z","iopub.status.idle":"2025-12-13T14:13:52.388692Z","shell.execute_reply.started":"2025-12-13T14:13:52.374863Z","shell.execute_reply":"2025-12-13T14:13:52.388107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Äá»c dá»¯ liá»‡u Song ngá»¯\nHÃ m Ä‘á»c Ä‘á»“ng thá»i hai file nguá»“n (Tiáº¿ng Anh) vÃ  Ä‘Ã­ch (Tiáº¿ng Viá»‡t), ghÃ©p chÃºng thÃ nh cÃ¡c cáº·p cÃ¢u song song. CÃ¡c cáº·p cÃ¢u bá»‹ rá»—ng hoáº·c lá»—i sáº½ bá»‹ loáº¡i bá» Ä‘á»ƒ khÃ´ng lÃ m nhiá»…u quÃ¡ trÃ¬nh huáº¥n luyá»‡n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 2: DATA READING FUNCTION ---\ndef read_parallel_files(src_filename, tgt_filename):\n    \"\"\"Äá»c cáº·p file song ngá»¯, tráº£ vá» list cÃ¡c tuple (cÃ¢u_nguá»“n, cÃ¢u_Ä‘Ã­ch)\"\"\"\n    # Kiá»ƒm tra Ä‘Æ°á»ng dáº«n (há»— trá»£ cáº£ thÆ° má»¥c hiá»‡n táº¡i vÃ  thÆ° má»¥c input cá»§a Kaggle)\n    possible_paths = [\"./\", \"/kaggle/input/\", \"/kaggle/working/\"]\n    \n    src_path, tgt_path = None, None\n    for p in possible_paths:\n        if os.path.exists(os.path.join(p, src_filename)):\n            src_path = os.path.join(p, src_filename)\n        if os.path.exists(os.path.join(p, tgt_filename)):\n            tgt_path = os.path.join(p, tgt_filename)\n            \n    if not src_path or not tgt_path:\n        print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file {src_filename} hoáº·c {tgt_filename}. Bá» qua.\")\n        return []\n\n    print(f\"ğŸ“– Äang Ä‘á»c: {src_path} vÃ  {tgt_path}\")\n    with open(src_path, 'r', encoding='utf-8') as f_src, \\\n         open(tgt_path, 'r', encoding='utf-8') as f_tgt:\n        src_lines = [clean_text(line.strip()) for line in f_src.read().splitlines()]\n        tgt_lines = [clean_text(line.strip()) for line in f_tgt.read().splitlines()]\n    \n    # Lá»c bá» cÃ¡c cáº·p cÃ¢u rá»—ng hoáº·c lá»‡ch dÃ²ng\n    pairs = []\n    min_len = min(len(src_lines), len(tgt_lines))\n    for i in range(min_len):\n        if src_lines[i] and tgt_lines[i]:\n            pairs.append((src_lines[i], tgt_lines[i]))\n            \n    return pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.389517Z","iopub.execute_input":"2025-12-13T14:13:52.389775Z","iopub.status.idle":"2025-12-13T14:13:52.402777Z","shell.execute_reply.started":"2025-12-13T14:13:52.389753Z","shell.execute_reply":"2025-12-13T14:13:52.402037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Táº£i vÃ  Chia táº­p dá»¯ liá»‡u\nLoad 3 táº­p dá»¯ liá»‡u chuáº©n tá»« bá»™ NLP VLSP:\n- **Train set:** DÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.\n- **Validation set (tst2012):** DÃ¹ng Ä‘á»ƒ tinh chá»‰nh vÃ  kiá»ƒm tra overfitting trong lÃºc train.\n- **Test set (tst2013):** DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™c láº­p sau cÃ¹ng.","metadata":{}},{"cell_type":"code","source":"# --- CELL 3: LOAD DATA ---\nprint(\"\\n--- ÄANG Táº¢I Dá»® LIá»†U ---\")\n# Äáº£m báº£o tÃªn file khá»›p vá»›i file báº¡n upload\ntrain_pairs = read_parallel_files(\"/kaggle/input/entovinlp/train.en\", \"/kaggle/input/entovinlp/train.vi\")\nval_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2012.en\", \"/kaggle/input/entovinlp/tst2012.vi\")\ntest_pairs = read_parallel_files(\"/kaggle/input/entovinlp/tst2013.en\", \"/kaggle/input/entovinlp/tst2013.vi\")\n\nprint(f\"âœ… Train size: {len(train_pairs)}\")\nprint(f\"âœ… Val size: {len(val_pairs)}\")\nprint(f\"âœ… Test size: {len(test_pairs)}\")\n\nif len(train_pairs) > 0:\n    print(f\"ğŸ” VÃ­ dá»¥ máº«u: {train_pairs[0:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:13:52.40469Z","iopub.execute_input":"2025-12-13T14:13:52.404954Z","iopub.status.idle":"2025-12-13T14:14:51.236442Z","shell.execute_reply.started":"2025-12-13T14:13:52.404939Z","shell.execute_reply":"2025-12-13T14:14:51.235694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Huáº¥n luyá»‡n BPE Tokenizer\nSá»­ dá»¥ng thuáº­t toÃ¡n **Byte Pair Encoding (BPE)** Ä‘á»ƒ mÃ£ hÃ³a vÄƒn báº£n.\n- GiÃºp giáº£i quyáº¿t váº¥n Ä‘á» tá»« vá»±ng má»Ÿ (Out-of-Vocabulary) báº±ng cÃ¡ch chia nhá» tá»« chÆ°a biáº¿t thÃ nh cÃ¡c subword.\n- KÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn (Vocab Size): 10,000 token cho má»—i ngÃ´n ngá»¯.","metadata":{}},{"cell_type":"code","source":"# --- CELL 4: TRAIN TOKENIZERS ---\nprint(\"\\n--- HUáº¤N LUYá»†N TOKENIZER ---\")\n\ndef train_bpe_tokenizer(texts, vocab_size=8000):\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n    tokenizer.decoder = decoders.ByteLevel()\n    \n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[UNK]\"],\n        show_progress=False\n    )\n    tokenizer.train_from_iterator(texts, trainer=trainer)\n    return tokenizer\n\n# Gá»™p text Ä‘á»ƒ train tokenizer\nall_src_text = [p[0] for p in train_pairs + val_pairs]\nall_tgt_text = [p[1] for p in train_pairs + val_pairs]\n\nif not all_src_text: # Dummy data náº¿u chÆ°a load Ä‘Æ°á»£c file\n    all_src_text = [\"Hello world\"]\n    all_tgt_text = [\"Xin chÃ o\"]\n\nen_tokenizer = train_bpe_tokenizer(all_src_text, vocab_size=10000)\nvi_tokenizer = train_bpe_tokenizer(all_tgt_text, vocab_size=10000)\n\n# Láº¥y ID cÃ¡c token Ä‘áº·c biá»‡t\nPAD_ID = en_tokenizer.token_to_id(\"[PAD]\")\nSTART_ID = vi_tokenizer.token_to_id(\"[START]\")\nEND_ID = vi_tokenizer.token_to_id(\"[END]\")\n\nprint(\"âœ… Tokenizer Ä‘Ã£ sáºµn sÃ ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:51.23734Z","iopub.execute_input":"2025-12-13T14:14:51.237712Z","iopub.status.idle":"2025-12-13T14:14:57.310542Z","shell.execute_reply.started":"2025-12-13T14:14:51.237683Z","shell.execute_reply":"2025-12-13T14:14:57.309854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XÃ¢y dá»±ng Dataset & Batching\n- **EnViDataset:** Class chá»‹u trÃ¡ch nhiá»‡m lÆ°u trá»¯ vÃ  truy xuáº¥t tá»«ng cáº·p cÃ¢u.\n- **Collate Function:** HÃ m xá»­ lÃ½ tá»«ng batch dá»¯ liá»‡u trÆ°á»›c khi Ä‘Æ°a vÃ o GPU:\n  - MÃ£ hÃ³a text sang ID sá»‘.\n  - ThÃªm token Ä‘áº·c biá»‡t `[START]` vÃ  `[END]` cho cÃ¢u Ä‘Ã­ch.\n  - **Padding:** ThÃªm token `[PAD]` Ä‘á»ƒ Ä‘á»™ dÃ i cÃ¡c cÃ¢u trong má»™t batch báº±ng nhau.","metadata":{}},{"cell_type":"code","source":"# --- CELL 5: DATASET CLASS ---\nclass EnViDataset(Dataset):\n    def __init__(self, pairs):\n        self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        return self.pairs[idx]\n\ndef collate_fn(batch):\n    en_batch, vi_batch = zip(*batch)\n    \n    # Encode tiáº¿ng Anh (Source)\n    en_enc = en_tokenizer.encode_batch(list(en_batch))\n    en_ids = [e.ids for e in en_enc]\n    \n    # Encode tiáº¿ng Viá»‡t (Target) - ThÃªm START vÃ  END thá»§ cÃ´ng\n    vi_ids = []\n    for text in vi_batch:\n        ids = vi_tokenizer.encode(text).ids\n        vi_ids.append([START_ID] + ids + [END_ID])\n    \n    # Padding\n    max_len_en = max([len(x) for x in en_ids])\n    max_len_vi = max([len(x) for x in vi_ids])\n    \n    padded_en = [x + [PAD_ID] * (max_len_en - len(x)) for x in en_ids]\n    padded_vi = [x + [PAD_ID] * (max_len_vi - len(x)) for x in vi_ids]\n    \n    return torch.tensor(padded_en), torch.tensor(padded_vi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.31126Z","iopub.execute_input":"2025-12-13T14:14:57.311593Z","iopub.status.idle":"2025-12-13T14:14:57.317842Z","shell.execute_reply.started":"2025-12-13T14:14:57.311573Z","shell.execute_reply":"2025-12-13T14:14:57.317178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Khá»Ÿi táº¡o DataLoader\nTáº¡o cÃ¡c iterator Ä‘á»ƒ náº¡p dá»¯ liá»‡u vÃ o mÃ´ hÃ¬nh theo tá»«ng Batch (kÃ­ch thÆ°á»›c 32).\n* `shuffle=True` cho táº­p Train Ä‘á»ƒ xÃ¡o trá»™n dá»¯ liá»‡u, giÃºp mÃ´ hÃ¬nh há»c tá»•ng quÃ¡t hÆ¡n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 6: DATALOADERS ---\nBATCH_SIZE = 32\n\ntrain_loader = DataLoader(EnViDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(EnViDataset(val_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\nprint(f\"DataLoaders created. Batch size: {BATCH_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.318602Z","iopub.execute_input":"2025-12-13T14:14:57.318812Z","iopub.status.idle":"2025-12-13T14:14:57.332413Z","shell.execute_reply.started":"2025-12-13T14:14:57.318786Z","shell.execute_reply":"2025-12-13T14:14:57.331847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CÃ¡c thÃ nh pháº§n Kiáº¿n trÃºc NÃ¢ng cao (Advanced Architecture)\nNhÃ³m sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t hiá»‡n Ä‘áº¡i (tÆ°Æ¡ng tá»± LLaMA) thay vÃ¬ Transformer cá»• Ä‘iá»ƒn:\n1. **Rotary Positional Embeddings (RoPE):** MÃ£ hÃ³a vá»‹ trÃ­ dáº¡ng xoay, giÃºp mÃ´ hÃ¬nh náº¯m báº¯t vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i tá»‘t hÆ¡n Sinusoidal truyá»n thá»‘ng.\n2. **SwiGLU Activation:** HÃ m kÃ­ch hoáº¡t thay tháº¿ ReLU, giÃºp luá»“ng gradient á»•n Ä‘á»‹nh vÃ  há»™i tá»¥ tá»‘t hÆ¡n.\n3. **Grouped Query Attention (GQA):** Tá»‘i Æ°u hÃ³a Multi-head Attention báº±ng cÃ¡ch chia sáº» Key/Value heads, giáº£m chi phÃ­ tÃ­nh toÃ¡n mÃ  váº«n giá»¯ Ä‘á»™ chÃ­nh xÃ¡c.","metadata":{}},{"cell_type":"code","source":"# --- CELL 7: MODEL COMPONENTS ---\n# --- Rotary Positional Embeddings ---\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(x, cos, sin):\n    return (x * cos) + (rotate_half(x) * sin)\n\nclass RotaryPositionalEncoding(nn.Module):\n    def __init__(self, head_dim, max_seq_len=2048):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n        t = torch.arange(max_seq_len).float()\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos\", emb.cos()[None, :, None, :])\n        self.register_buffer(\"sin\", emb.sin()[None, :, None, :])\n\n    def forward(self, x, seq_len):\n        return self.cos[:, :seq_len, :, :], self.sin[:, :seq_len, :, :]\n\n# --- Feed Forward (SwiGLU) ---\nclass SwiGLU(nn.Module):\n    def __init__(self, hidden_dim, intermediate_dim):\n        super().__init__()\n        self.w1 = nn.Linear(hidden_dim, intermediate_dim)\n        self.w2 = nn.Linear(hidden_dim, intermediate_dim)\n        self.w3 = nn.Linear(intermediate_dim, hidden_dim)\n\n    def forward(self, x):\n        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n\n# --- GQA Attention ---\nclass GQA(nn.Module):\n    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_dim = hidden_dim // num_heads\n        self.num_groups = num_heads // num_kv_heads\n        \n        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.k_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n        self.v_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = dropout\n\n    def forward(self, x, enc_out=None, mask=None, rope_cos=None, rope_sin=None):\n        batch, seq_len, _ = x.shape\n        kv_input = enc_out if enc_out is not None else x\n        kv_seq_len = kv_input.shape[1]\n\n        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)\n        k = self.k_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n        v = self.v_proj(kv_input).view(batch, kv_seq_len, self.num_kv_heads, self.head_dim)\n\n        if rope_cos is not None and enc_out is None:\n            q = apply_rotary_pos_emb(q, rope_cos, rope_sin)\n            k = apply_rotary_pos_emb(k, rope_cos, rope_sin)\n\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if self.num_groups > 1:\n            k = k[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n            v = v[:, :, None, :, :].expand(batch, self.num_kv_heads, self.num_groups, kv_seq_len, self.head_dim).reshape(batch, self.num_heads, kv_seq_len, self.head_dim)\n\n        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0)\n        return self.o_proj(out.transpose(1, 2).reshape(batch, seq_len, -1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.333208Z","iopub.execute_input":"2025-12-13T14:14:57.333937Z","iopub.status.idle":"2025-12-13T14:14:57.352787Z","shell.execute_reply.started":"2025-12-13T14:14:57.333914Z","shell.execute_reply":"2025-12-13T14:14:57.35212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Láº¯p rÃ¡p MÃ´ hÃ¬nh Transformer\nKáº¿t há»£p cÃ¡c thÃ nh pháº§n trÃªn thÃ nh kiáº¿n trÃºc Encoder-Decoder hoÃ n chá»‰nh:\n* **Encoder:** MÃ£ hÃ³a cÃ¢u tiáº¿ng Anh thÃ nh vector ngá»¯ nghÄ©a.\n* **Decoder:** Sinh cÃ¢u tiáº¿ng Viá»‡t tá»« vector ngá»¯ nghÄ©a, sá»­ dá»¥ng cÆ¡ cháº¿ Cross-Attention Ä‘á»ƒ \"nhÃ¬n\" láº¡i cÃ¢u nguá»“n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 8: TRANSFORMER MODEL ---\nclass TransformerBlock(nn.Module):\n    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1, is_decoder=False):\n        super().__init__()\n        self.norm1 = nn.RMSNorm(hidden_dim)\n        self.attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n        self.is_decoder = is_decoder\n        if is_decoder:\n            self.norm2 = nn.RMSNorm(hidden_dim)\n            self.cross_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n        self.norm_ffn = nn.RMSNorm(hidden_dim)\n        self.ffn = SwiGLU(hidden_dim, hidden_dim * 4)\n\n    def forward(self, x, enc_out=None, mask=None, cross_mask=None, rope_cos=None, rope_sin=None):\n        x = x + self.attn(self.norm1(x), mask=mask, rope_cos=rope_cos, rope_sin=rope_sin)\n        if self.is_decoder:\n            x = x + self.cross_attn(self.norm2(x), enc_out=enc_out, mask=cross_mask)\n        x = x + self.ffn(self.norm_ffn(x))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4):\n        super().__init__()\n        self.src_emb = nn.Embedding(src_vocab, hidden_dim)\n        self.tgt_emb = nn.Embedding(tgt_vocab, hidden_dim)\n        self.rope = RotaryPositionalEncoding(hidden_dim // num_heads)\n        self.encoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads) for _ in range(num_layers)])\n        self.decoders = nn.ModuleList([TransformerBlock(hidden_dim, num_heads, num_kv_heads, is_decoder=True) for _ in range(num_layers)])\n        self.final_norm = nn.RMSNorm(hidden_dim)\n        self.fc_out = nn.Linear(hidden_dim, tgt_vocab)\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        x = self.src_emb(src)\n        rope_cos, rope_sin = self.rope(x, x.shape[1])\n        for layer in self.encoders:\n            x = layer(x, mask=src_mask, rope_cos=rope_cos, rope_sin=rope_sin)\n        enc_out = x\n        \n        x = self.tgt_emb(tgt)\n        rope_cos_tgt, rope_sin_tgt = self.rope(x, x.shape[1])\n        for layer in self.decoders:\n            x = layer(x, enc_out=enc_out, mask=tgt_mask, cross_mask=src_mask, rope_cos=rope_cos_tgt, rope_sin=rope_sin_tgt)\n        return self.fc_out(self.final_norm(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.353411Z","iopub.execute_input":"2025-12-13T14:14:57.353665Z","iopub.status.idle":"2025-12-13T14:14:57.370367Z","shell.execute_reply.started":"2025-12-13T14:14:57.353643Z","shell.execute_reply":"2025-12-13T14:14:57.36971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thiáº¿t láº­p Huáº¥n luyá»‡n\n- **Masking:** Táº¡o che (mask) Ä‘á»ƒ Ä‘áº£m báº£o Decoder khÃ´ng \"nhÃ¬n tháº¥y\" tÆ°Æ¡ng lai khi dá»± Ä‘oÃ¡n tá»« tiáº¿p theo.\n- **Label Smoothing (0.1):** Ká»¹ thuáº­t Regularization giÃºp mÃ´ hÃ¬nh bá»›t tá»± tin thÃ¡i quÃ¡ vÃ o dá»¯ liá»‡u train, giáº£m Overfitting.\n- **Optimizer:** AdamW (biáº¿n thá»ƒ cá»§a Adam vá»›i Weight Decay tÃ¡ch biá»‡t) giÃºp tá»‘i Æ°u tá»‘t hÆ¡n.","metadata":{}},{"cell_type":"code","source":"# --- CELL 9: INIT TRAINING ---\ndef create_masks(src, tgt):\n    src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n    batch, seq_len = tgt.shape\n    causal = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n    tgt_pad = (tgt == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n    return src_mask, causal + tgt_pad\n\nmodel = Transformer(\n    src_vocab=en_tokenizer.get_vocab_size(),\n    tgt_vocab=vi_tokenizer.get_vocab_size(),\n    hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n\nprint(\"Model Initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:14:57.371004Z","iopub.execute_input":"2025-12-13T14:14:57.371268Z","iopub.status.idle":"2025-12-13T14:15:00.252982Z","shell.execute_reply.started":"2025-12-13T14:14:57.371205Z","shell.execute_reply":"2025-12-13T14:15:00.25222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# QuÃ¡ trÃ¬nh Huáº¥n luyá»‡n (Training Loop)\nCháº¡y huáº¥n luyá»‡n qua 10 Epochs. Táº¡i má»—i Epoch:\n* TÃ­nh Loss trÃªn táº­p Train.\n* ÄÃ¡nh giÃ¡ ngay trÃªn táº­p Validation Ä‘á»ƒ theo dÃµi sá»± há»™i tá»¥.\n* LÆ°u láº¡i Model sau khi hoÃ n táº¥t.","metadata":{}},{"cell_type":"code","source":"# --- CELL 10: TRAINING LOOP ---\nEPOCHS = 10\nprint(\"\\n--- Báº®T Äáº¦U HUáº¤N LUYá»†N ---\")\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for src, tgt in pbar:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n        src_mask, tgt_mask = create_masks(src, tgt_input)\n        \n        optimizer.zero_grad()\n        output = model(src, tgt_input, src_mask, tgt_mask)\n        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        \n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for src, tgt in val_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n            src_mask, tgt_mask = create_masks(src, tgt_input)\n            output = model(src, tgt_input, src_mask, tgt_mask)\n            val_loss += criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1)).item()\n            \n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n\ntorch.save(model.state_dict(), \"transformer_en_vi.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T14:15:00.253837Z","iopub.execute_input":"2025-12-13T14:15:00.254194Z","iopub.status.idle":"2025-12-13T15:27:57.272211Z","shell.execute_reply.started":"2025-12-13T14:15:00.254162Z","shell.execute_reply":"2025-12-13T15:27:57.271409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kiá»ƒm thá»­ (Inference)\nHÃ m dá»‹ch sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p **Greedy Decoding**: Táº¡i má»—i bÆ°á»›c, chá»n tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t lÃ m tá»« tiáº¿p theo cho báº£n dá»‹ch.","metadata":{}},{"cell_type":"code","source":"# --- CELL 11: INFERENCE ---\nprint(\"\\n--- TEST Káº¾T QUáº¢ ---\")\ndef translate(sentence, max_len=100):\n    model.eval()\n    with torch.no_grad():\n        src = torch.tensor([en_tokenizer.encode(sentence).ids]).to(device)\n        src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n        tgt_ids = [START_ID]\n        \n        for _ in range(max_len):\n            tgt = torch.tensor([tgt_ids]).to(device)\n            causal = torch.triu(torch.full((tgt.shape[1], tgt.shape[1]), float('-inf'), device=device), diagonal=1)\n            out = model(src, tgt, src_mask, causal)\n            next_token = out[0, -1, :].argmax().item()\n            if next_token == END_ID: break\n            tgt_ids.append(next_token)\n        return vi_tokenizer.decode(tgt_ids[1:])\n\nfor i in range(5):\n    if len(test_pairs) > 0:\n        idx = random.randint(0, len(test_pairs)-1)\n        en_txt, vi_txt = test_pairs[idx]\n        print(f\"ğŸ”¹ Input:  {en_txt}\\nğŸ”¸ Target: {vi_txt}\\nğŸš€ Model:  {translate(en_txt)}\\n{'-'*50}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:27:57.273337Z","iopub.execute_input":"2025-12-13T15:27:57.27408Z","iopub.status.idle":"2025-12-13T15:27:58.35176Z","shell.execute_reply.started":"2025-12-13T15:27:57.27406Z","shell.execute_reply":"2025-12-13T15:27:58.351153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng (Metric Evaluation)\nSá»­ dá»¥ng **BLEU Score** (thÃ´ng qua thÆ° viá»‡n `sacrebleu`) - tiÃªu chuáº©n vÃ ng trong Ä‘Ã¡nh giÃ¡ dá»‹ch mÃ¡y.\n * Äiá»ƒm BLEU Ä‘Æ°á»£c tÃ­nh trÃªn táº­p Test (hoÃ n toÃ n má»›i vá»›i mÃ´ hÃ¬nh).\n * Káº¿t quáº£ pháº£n Ã¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a vÄƒn báº£n mÃ¡y dá»‹ch vÃ  vÄƒn báº£n máº«u cá»§a con ngÆ°á»i.","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:27:58.442004Z","iopub.execute_input":"2025-12-13T15:27:58.442513Z","iopub.status.idle":"2025-12-13T15:28:02.798777Z","shell.execute_reply.started":"2025-12-13T15:27:58.442495Z","shell.execute_reply":"2025-12-13T15:28:02.797767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sacrebleu\nimport random\nfrom tqdm import tqdm # Thanh hiá»ƒn thá»‹ tiáº¿n Ä‘á»™\n\ndef calculate_bleu(data_pairs, num_samples=100):\n    print(f\"--- ğŸ“Š ÄANG TÃNH ÄIá»‚M BLEU TRÃŠN {num_samples} MáºªU ---\")\n    \n    # Chá»n ngáº«u nhiÃªn máº«u Ä‘á»ƒ test (hoáº·c láº¥y háº¿t náº¿u num_samples=None)\n    if num_samples is not None and num_samples < len(data_pairs):\n        samples = random.sample(data_pairs, num_samples)\n    else:\n        samples = data_pairs\n\n    preds = [] # CÃ¡c cÃ¢u mÃ¡y dá»‹ch\n    refs = []  # CÃ¡c cÃ¢u Ä‘Ã¡p Ã¡n chuáº©n\n\n    # Báº¯t Ä‘áº§u dá»‹ch\n    for en_txt, vi_txt in tqdm(samples):\n        # Dá»‹ch cÃ¢u tiáº¿ng Anh\n        pred_sent = translate(en_txt)\n        \n        preds.append(pred_sent)\n        refs.append(vi_txt) # Sacrebleu nháº­n list cÃ¡c string cho refs\n\n    # TÃ­nh Ä‘iá»ƒm BLEU\n    # refs cáº§n Ä‘Æ°á»£c bá»c trong list vÃ¬ 1 cÃ¢u input cÃ³ thá»ƒ cÃ³ nhiá»u cÃ¢u target (á»Ÿ Ä‘Ã¢y ta cÃ³ 1)\n    bleu = sacrebleu.corpus_bleu(preds, [refs])\n    \n    return bleu.score\n\n# --- CHáº Y TÃNH ÄIá»‚M ---\n# Báº¡n cÃ³ thá»ƒ tÄƒng sá»‘ lÆ°á»£ng máº«u lÃªn len(test_pairs) Ä‘á»ƒ chÃ­nh xÃ¡c hÆ¡n (sáº½ cháº¡y lÃ¢u hÆ¡n)\nscore = calculate_bleu(test_pairs, num_samples= None)\n\nprint(f\"\\nğŸŒŸ ÄIá»‚M BLEU Cá»¦A MODEL: {score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:40:04.758742Z","iopub.execute_input":"2025-12-13T15:40:04.759044Z","iopub.status.idle":"2025-12-13T15:45:36.947251Z","shell.execute_reply.started":"2025-12-13T15:40:04.759024Z","shell.execute_reply":"2025-12-13T15:45:36.946704Z"}},"outputs":[],"execution_count":null}]}